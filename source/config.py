#
# BA ML FS17 - Dirk von Gr√ºnigen & Martin Weilenmann
#
# Description: This module contains the Config class
#              which is responsible for holding the
#              configuration while running the project.
#

import json
import time

from os import path

class Config(object):
    '''Config class for holding the configuration parameters.'''

    # Path constants
    ROOT_PATH    = path.realpath(path.join(path.dirname(__file__), '..'))
    LOGS_PATH    = path.join(ROOT_PATH, 'logs')
    DATA_PATH    = path.join(ROOT_PATH, 'data')
    CONFIG_PATH  = path.join(ROOT_PATH, 'configs')
    RESULTS_PATH = path.join(ROOT_PATH, 'results')

    # Vocabulary constants
    UNKNOWN_WORD_IDX = 0

    # Contains the defaults which are applied in case any config
    # parameter is missing. ALL parameters should have an entry
    # in this list. If no meaningful value can be defined beforehand,
    # None should be used as the value.
    DEFAULT_PARAMS = {
        # Id of the experiment (autogenerated)
        'id': None,

        # Signifies the git version used when running the experiment
        'git_rev': None,

        # Random seed used to initialize numpy and tensorflow rngs
        'random_seed': 1337,

        # Path to the model to load (optional). The path MUST point
        # to a *.chkp file, not any of the related files (*.index, *.meta, etc.).
        # This means that the path might be "directory/model-1.chkp-1001" even though
        # this file does not exist on the hard drive.
        'model_path': None,

        # Name of the experiment (optional)
        'name': None,

        # Number of epochs to do while training
        'epochs': 1,

        # Batch size which will be used when training
        'batch_size': 8,

        # Defines how much batches will be considered per epoch.
        'batches_per_epoch': 100,

        # Number of layers (or cells) in the encoder/decoder
        # parts of the network
        'num_encoder_layers': 1,
        'num_decoder_layers': 1,

        # Number of hidden units in each of the layers in each cell
        'num_hidden_units': 100,

        # Defines the cell type which will be used, either 'RNN', 'LSTM' or 'GRU'
        'cell_type': 'LSTM',

        # Defines the used vocabulary
        'vocabulary': None,

        # Defines the path to the word2vec embeddings to load (if used)
        'w2v_embeddings': None,

        # Defines the path to the fasttext embeddings to load (if used)
        'ft_embeddings': None,

        # Defines wether the used RNN processes the data in both directions
        # (backward/forward) or if it should be unidirectional.
        'bidirectional': False,

        # Defines wether the attention mechanism should be activated in the
        # decoder part of the sequence-to-sequence model.
        'use_attention': False,

        # Defines wether the input sample should be reversed before it's
        # fed to the encoder. This is a trick used in the original seq2seq
        # paper.
        'reverse_input': False,

        # Defines wether the dropout mechanism should be used to mitigate
        # the problem of overfitting.
        'use_dropout': False,

        # Defines wether the debuggin environment should be activated.
        'debug': False,

        # Defines how much words should be considered in the vocabulary
        # in case no embeddings are provided.
        'max_vocabulary_size': 10000,

        # Defines how much dimensions should be used when using random embeddings.
        'max_random_embeddings_size': 10,

        # Defines the optimizer and the hyperparameters to use
        'optimizer_name': 'AdaDelta',
        'optimizer_parameters': {},

        # Defines how much checkpoints should be kept at max. Defaults to 5.
        'checkpoint_max_to_keep': 5,

        # Defines how much of the input neurons should still be considered
        # when applying the dropout mechanism. The value 1.0 means that all
        # neurons are used and none is dropped.
        'dropout_input_keep': 1.0,

        # Defines how much of the output neurons should still be considered
        # when applying the dropout mechanism. The value 1.0 means that all
        # neurons are used and none is dropped.
        'dropout_output_keep': 1.0,

        # Defines which tokenizer should be used to parse the conversational
        # texts. The default tokenizers is the 'word_tokenizer' of the ntlk
        # module.
        'tokenizer_name': 'word_tokenize',

        # Defines which training data to load. It tries to load the conversation
        # of the file at the given path, this file must obey the correct format
        # described in the DataLoader class.
        'training_data': None,

        # Defines which test data to load. It tries to load the conversation
        # of the file at the given path, this file must obey the correct format
        # described in the DataLoader class.
        'test_data': None,

        # Defines the maximum allowed length of the input sentences. Longer inputs
        # will be reduced to this maximum.
        'max_input_length': 100
    }

    def __init__(self, cfg_obj={}):
        '''Creates a new Config object with the parameters
           from the cfg_obj parameter. The default parameters
           are used if no cfg_obj object is given.'''
        self.cfg_obj = self.DEFAULT_PARAMS.copy()
        self.cfg_obj.update(cfg_obj)
        self.__create_id()

    def get(self, name):
        '''Returns the value for the given name stored in the current config object.'''
        if name in self.cfg_obj:
            return self.cfg_obj[name]
        else:
            raise Exception('there is no value for the name %s in the config' % name)

    def set(self, name, value):
        '''Sets the value for the given name in the current config object.'''
        if name in self.cfg_obj:
            self.cfg_obj[name] = value
        else:
            raise Exception('there is no value for the name %s in the config' % name)

    @staticmethod
    def load_from_json(json_path):
        '''Loads the JSON config from the given path and
           creates a Config object for holding the parameters.'''
        with open(json_path, 'r') as f:
            return Config(json.load(f))

    def __create_id(self):
        '''This method is responsible for creating a unique id
           for the loaded configuration. This id is then used to
           create the results directory.'''
        name = self.get('name')
        timestamp = time.strftime('%Y-%m-%d_%H-%M-%S')

        if name:
            timestamp = '%s_%s' % (timestamp, name)

        self.set('id', timestamp)
