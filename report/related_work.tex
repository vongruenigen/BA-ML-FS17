\chapter{Related Work}
\label{related_work}

The history of dialog systems and chatbots dates far more back than most people would probably expect, as the first known dialog system, called \emph{ELIZA}, was already published in the year 1966 by Joseph Weizenbaum~\cite{Weizenbaum:1966}. Even thought this system was pretty primitive from a todays standpoint - it used scripted responses instead of learning by to talk by itself - it had a huge impact and was considered one of the first contenders for passing the Turing test. The story then goes on with PARRY, which was a chatbot simulating a patient with paranoid schizophrenia~\cite{Colby:1974}. An interesting note here is that it was the first actual computer program to almost pass a special variant of the Turing test as only 48\% of the participants in the test classified it correctly to be a machine. It took much more time and several other implementations of chatbots, such as Jabberwacky, Dr. Sbaitso, A.L.I.C.E and SmarterChild until we arrive at todays most sophisticated systems. The biggest problems with these systems was that they did not actually learn anything, but were instead driven by a rule-based framework. Maybe the best known, intelligent dialog system is the IBM Watson system~\cite{Ferrucci:2012}, which has shown its performance several times by beating the world champions in the question-answering game \emph{Jeopardy!}. It is built by combining several, complicated subsystems, each responsible for a certain task (e.g. language understanding, information retrieval, answer ranking). Since the rise of deep learning within the last century, plenty of different ways to build dialog systems have been proposed.

While research for out thesis, we stumbled across the paper \emph{Neural Conversational Model} by Oriol Vinyals and Quoc V. Le~\cite{Vinyals:2015}, which immediately got our attention for several reasons: First, it was one of the few systems using an end-to-end approach, which means that there is only a single component responsible for the understanding and processing the input and for generating of the responses. We found that an appealing property because it lowers the overall complexity of such systems. On the other hand, we were also interested because it not only use a single neural network, but also a fairly new architecture, called \emph{Sequence To Sequence} learning, or seq2seq in short. This architecture was proposed by several different people around the same time~\cite{Sutskever:2014}\cite{Kalchbrenner:2013}\cite{Cho:2014}, mainly as an alternative for statistical machine translation. It was quickly noticed afterwards, that the proposed architecture could not only be used for machine translations, but also for a lot of other tasks which need to map input to output sequences, such as image captioning~\cite{Xu:2015}, syntactic constituency parsing~\cite{Vinyals:2015:Foreign} or even chatbots~\cite{Zivkovic:Trumpbot}\cite{Lison:2016}. There are also some unsupervised use-cases, such as generating embedded representations for videos~\cite{Nitish:2015}.

In this thesis, instead of trying to work on the architecture, we want to evaluate if the results found in the \emph{Neural Conversational Model} paper can be replicated by a much smaller system, or to be precise, a system which is half the size of the original one. Our goal is to fit the whole model onto a single GPU for training and inference instead of using only CPUs as they did in the paper. We then try to analyze the development of the models over a prolonged time of training, with a special eye on the language model, as this model need a lot of time to train. As the last point, we are going to investigate into how beneficial other additions, such as attention and beam-search, are for the resulting models.