\chapter{Related Work}
\label{related_work}

The history of dialog systems and chatbots dates further back than most people would probably expect, as the first known dialog system, called ELIZA, was already published in the year 1966 by Joseph Weizenbaum~\cite{Weizenbaum:1966}. Even though this system was pretty primitive from a todays standpoint - it used scripted responses instead of learning to talk - it was widely recognized and was considered one of the first contenders to pass the Turing test. One of its successors was then PARRY, which was a chatbot simulating a patient with paranoid schizophrenia~\cite{Colby:1974}. An interesting note here is that it was the first actual computer program to almost pass a special variant of the Turing test as only 48\% of the participants involved in the test classified it correctly to be a machine. It took many years and several other implementations of chatbots, such as Jabberwacky, Dr. Sbaitso, A.L.I.C.E and SmarterChild until today's highly sophisticated systems became available. The biggest problem with most of these earlier systems was that they did not actually learn anything, but were driven by rule-based frameworks instead. Maybe the best known, intelligent dialog system is the IBM Watson system~\cite{Ferrucci:2012}, which has shown its performance several times by beating the world champions in the question-answering game \emph{Jeopardy!}. It is built by combining several, complicated subsystems, each responsible for a certain task (e.g. language understanding, information retrieval, answer ranking). Since the rise of deep learning within the last century, ample of different ways to build dialog systems have been proposed.

During the research for our thesis, we found the paper ``Neural Conversational Model'' by Oriol Vinyals and Quoc V. Le~\cite{Vinyals:2015}, which immediately got our attention for several reasons: First, it was one of the few systems using an end-to-end approach, which means that a single closed system is responsible for the understanding and processing the input as well as for the generation of the responses. We found that an appealing property because it helps to lower the costs of management, design and integration. On the other hand, we were also interested because it not only uses a single neural network, but also a fairly new architecture, called \emph{Sequence-To-Sequence} learning, or seq2seq in short. This architecture was proposed by several different researchers around the same time~\cite{Sutskever:2014}\cite{Kalchbrenner:2013}\cite{Cho:2014}, mainly as an alternative for statistical machine translation. It was quickly discovered afterwards, that the proposed architecture could not only be used for machine translations, but also for various other tasks requiring to map input to output sequences, such as image captioning~\cite{Xu:2015}, syntactic constituency parsing~\cite{Vinyals:2015:Foreign} and dialog systems or chatbots~\cite{Zivkovic:Trumpbot}\cite{Lison:2016}. There are also some unsupervised use-cases for seq2seq models, such as generating embedded representations of videos~\cite{Nitish:2015}.

In this thesis, we want to evaluate if the results found in the \emph{Neural Conversational Model} paper can be replicated by a much smaller system, or to be precise, a model which is half the size of the original one. Our objective is to fit the entire model onto a single GPU for training and inference instead of an entire server with multiple CPUs as they did in the paper. We then try to analyze the development of the models over a prolonged time of training, as this model need a lot of time to train. As the last point, we are going to investigate how beneficial other extensions for the resulting models are, such as attention and beam-search.