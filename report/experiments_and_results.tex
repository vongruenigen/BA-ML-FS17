\chapter{Analysis of Results}
\todo{Diese Seite habe ich noch nicht genau angeschaut, aber ich denke wir geben hier auch unsere grobe struktur bekannt: Satz/Testset, n-Gramm, Wörter, thought Vektor. Von Ganz aussen nach innen, sozusagen.}
In this chapter, we are going to analyze the evolution and final results of the two models (see chapter \ref{methods:both_models}), trained on the OpenSubtitles and Reddit corpus, under several different aspects. This includes the analysis of the learning process throughout the training. Then we are going to analyse the language model of the trained models and try to find a correlation between the language model in the training dataset and the language model the trained models produce.\todo{Two times model is ugly!} In the end, we are going to focus on how well the model understands and percepts them, combined with an analysis

\begin{enumerate}[noitemsep]
	\item \textbf{Quantitive}: This part focuses on evaluating metrics and other measurements. We are going to look into the metrics at train time, a similarity analysis using Sent2Vec and an analysis of the produced language models by using n-gram collocations.\todo{Not correct: What are we going to do with ngrams?}
	\item \textbf{Qualitative}: This part focuses on the quality of the language produced by the models. This includes a comparison between our models and \emph{CleverBot}\footnote{http://www.cleverbot.com/} and the development of the language model of the models throughout the training time.
	\item \textbf{Miscellaneous}: This part focuses on theoretical analysis, such as analyzing the internal embeddings for the input sequence or if and how the models are using the soft-attention mechanism in their favor.
\end{enumerate}

The analysis is done in the order listed above, that means we start by analysing the quantitive measures
\blindtext

* Attention
* Learning process
* Generated outputs over time
* Generation at the end and comparison with cleverbot and paper
* Generated outputs at the end and related ngram analysis
* Generated embeddings for input sentences
* Sent2Vec analysis

\section{How Did The Training Go?}\todo{vielleicht müssen wir den anpassen wegen neuer struktur}
First, let us start by analysing the evolution of the models with regard to the available performance metrics throughout the time span of the training. Below, in Figures~\ref{results:learning_process:metrics:opensubtitles} and~\ref{results:learning_process:metrics:reddit}, one can see the development of the loss and perplexity on the training sets for the two different models.

\paragraph{The Opensubtitle and his variance} stechen bei der Betrachtung der loss ins Auge. is eye-catching when comparing the two is that the OpenSubtitles seems to have much more variance in its performance on the validation set as opposed to the Reddit model. This probably comes from the fact that the OpenSubtitles datasets are much more noisey than the one of the Reddit model, as \cite{Vinyals:2015} also already noticed. This has to do with the fact, that we do not have any information about turn taking, which means it is certainly possible that consecutive utterances may be uttered by the same person even though we treat it if it were two persons. Also, there is the problematic with the time lags between utterances as analysed in Chapter~\ref{data:opensubtitles:time_lag_analysis}. In contrast, with the Reddit dataset, we always know who uttered a comment and hence can build a dataset which ensures that the dialogs make sense from a structural perspective.
\paragraph{The Dips in the Reddit} Das Lernverhalten von reddit weist ebenfalls Besonderheit auf. Die loss Werte der trainingsdaten ziemlich genau alle 400k stark nach unten. Wir können uns dieses Verhalten "noch" nicht erklären. Ebenfalls alle 400k fallen die loss Werte der Validierungsdaten, um dann anschliessend langsam wieder zu steigen. Dieses Verhalten können wir für das gesamte Training beobachten. Wie bereits erwähnt, kennen wir die Ursache für diese 400k Schritte nicht.\todo{400k für uns im kopf behalten, vielleicht finden wir es ja noch heraus.}
\begin{figure}[H]
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/train_metrics.png}
	\caption{Development of the loss and perplexity on the training and validation set throughout the training of the OpenSubtitles model.}
	\label{results:learning_process:metrics:opensubtitles}
\end{figure}\todo{Einheiten angeben (x = 1000-er)}

\begin{figure}[H]
	\includegraphics[width=\linewidth]{img/plots/reddit/train_metrics.png}
	\caption{Development of the loss and perplexity on the training and validation set throughout the training of the Reddit model.}
	\label{results:learning_process:metrics:reddit}
\end{figure}\todo{Einheiten angeben (x = 1000-er)}

\paragraph{Compared to each other}\todo{können wir auch weglassen diesen paragraphen. Weiss nicht welche Erkenntnis wir daraus ziehen können} In der Abbildung \ref{} sehen wir die loss Werte auf den Vailidierungsdaten beider Modell in einem Bild. Hier das Bild mit der valid los/perplex beider Modelle referenzieren und kurz schreiben, wer wie schnell gelernt hat und wie sich die Kurve unterscheiden.

\paragraph{In general, the training seems successful}, die loss funktion reduzierte sich auf den Trainingsdaten, wie auch auf den Validierungsdaten. Daraus schliessen wir, dass der Trainingsprozess erfolgreich war. Wir sahen Auffälligkeiten und Unterschiede, wie die Modelle gelernt haben, können aber noch keine weiteren Schlüsse daraus ziehen. Wir möchten in der nächsten Section die Performanz der Modelle anhand unserer Testdaten testen.

\section{Performance when Testing the Models}
After we have seen that the training process looks fine, we are going to asses the performance of these models on our test datasets. Here we use the same metrics as within the training, namely the loss and perplexity values. We have evaluated each model on the respective test dataset for each checkpoint we have created while training (see Chapter~\ref{methods}). The resulting values are visualized in the Figures~\ref{result:test_performance:opensubtitles} and~\ref{result:test_performance:reddit}.

\paragraph{Surprisingly the results} on the test set are quite the opposite of the results from the training process. We did not expect that, but nevertheless, we are going to analyse the problem.
The results of the OpenSubtitles model seem to vary across the different checkpoints, with the best result having a perplexity of $71.07$ and a loss of $6.15$ and coming from the evaluation with the first checkpoint. The best result of the Reddit model is also achieved on the first checkpoint, with all other checkpoints having a worse perplexity.
This result stands in contradiction to what we have expected. Instead of 
\begin{itemize}
	\item Loss Werte varieren stark
	\item Verhalten sich überhaupt nicht ähnlich wie valid los/perplex (grafik mit valid/test vergliech?) 
	\item Ist loss die richtige Metrik für unseren Task? Gemäss metrik dürfte sich der Output nicht verbessert haben.
\end{itemize}

\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/test_metrics_both.png}
	\centering
	\small
	\text{The loss and perplexity.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/test_metrics_loss.png}
	\centering
	\small
	\text{Only the loss.}
	\endminipage\hfill
	\caption{The loss and perplexity by running the six different snapshots against the test dataset using the OpenSubtitles model.}
	\label{result:test_performance:opensubtitles}
\end{figure}

\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/test_metrics_both.png}
	\centering
	\small
	\text{The loss and perplexity.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/test_metrics_loss.png}
	\centering
	\small
	\text{Only the loss.}
	\endminipage\hfill
	\caption{The loss and perplexity by running the six different snapshots against the test dataset using the Reddit model.}
	\label{result:test_performance:reddit}
\end{figure}

\paragraph{However, it improved....} die loss/perplex auf dem testset suggerieren wie vorhergehend besprochen, dass er keine Fortschritte gemacht hat. Wenn wir jedoch Beispielsätze testen, sehen wir sehr wohl eine positive Entwicklung (hier das erste mal erwähnen, dass mehrmals über den gleichen corpus bei reddit vielleicht keine gute idee war, \ref{results:example_output1:reddit} gibt beim letzten checkpoint wieder eine schlechte frühere antwort "the movie"). Beispiele für Verbesserung des Sprachmodelles:(\ref{results:example_output1:reddit}, \ref{results:example_output2:reddit}, \ref{results:example_output1:OpenSubtitle}, \ref{results:example_output2:OpenSubtitle}) Wie in diesen Beispielen ersichtlich, findet offensichtlich eine Verbesserung der Antwort statt. Gemäss den loss werten auf den Testdaten, hätten die Resultate sogar schlechter werden müssen. Wir schliessen daraus, dass loss/perplex als metrik nicht gut ist/aussagekräftig. Das Problem dabei, wir trainierten auf loss/perplex. Wenn loss nicht geeignet wäre, müsste man auch während dem training andere metrik verwenden. Es gibt nicht die eine Metrik für Dialogsysteme. Deshalb entschieden wir  uns Sent2vec auszuprobieren.

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Speaker & Utterance\\ \midrule
			Input: 	& \texttt{Do you like Joffrey from game of thrones?}\\
			0.5M: 	& \texttt{i m not sure if you re a fan of the movie ,}\\
			& \texttt{but i m not sure if you re a fan of the movie .}\\
			1.0M: 	& \texttt{i m not a fan of the game , but i m not sure if it s a game or not .}\\
			1.5M:	& \texttt{i do nt know what i m talking about .}\\
			2.0M:	& \texttt{i do nt }\\
			2.5M:	& \texttt{i do nt know what i m talking about .}\\
			3.0M:	& \texttt{i do nt know , i just do nt like him .}\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Example dialog with the Reddit model, one response for each checkpoint.}
	\label{results:example_output1:reddit}
\end{table}

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Speaker & Utterance\\ \midrule
			Input: 	& \texttt{Which movie can you recommend me?}\\
			0.5M: 	& \texttt{the}\\
			1.0M: 	& \texttt{the movie .}\\
			1.5M:	& \texttt{the one with the}\\
			2.0M:	& \texttt{the dark knight rises}\\
			2.5M:	& \texttt{the prestige}\\
			3.0M:	& \texttt{the movie}\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Example dialog with the Reddit model, one response for each checkpoint.}
	\label{results:example_output2:reddit}
\end{table}

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Speaker & Utterance\\ \midrule
			Input: 	& \texttt{Are you a man or a woman?}\\
			0.5M: 	& \texttt{no , no , no}\\
			1.0M: 	& \texttt{no}\\
			1.5M:	& \texttt{i m not a man}\\
			2.0M:	& \texttt{i m not a woman}\\
			2.5M:	& \texttt{i m a man}\\
			3.0M:	& \texttt{i m not a woman}\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Example dialog with the Reddit model, one response for each checkpoint.}
	\label{results:example_output1:OpenSubtitle}
\end{table}

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Speaker & Utterance\\ \midrule
			Input: 	& \texttt{Why should it not be a good idea to improve you?}\\
			0.5M: 	& \texttt{no}\\
			1.0M: 	& \texttt{i don t know}\\
			1.5M:	& \texttt{because i love you}\\
			2.0M:	& \texttt{because i m a good man}\\
			2.5M:	& \texttt{i m just trying to make a good decision}\\
			3.0M:	& \texttt{i m not a good idea}\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Example dialog with the Reddit model, one response for each checkpoint.}
	\label{results:example_output2:OpenSubtitle}
\end{table}

\subsection{Sent2Vec Analysis}
As described in Chapter~\ref{fundamentals:sent2vec_test}
Ergebnisse für die beiden Sent2Vec Embeddings, pro Modell \ref{results:sent2vec:opensubtitles:results} und \ref{results:sent2vec:reddit:result}. Ersichtlich die Cos Distanz auf der y Achse und auf der X-Achse der Fortschritt des Trainings.

\paragraph{The semantic accuracy } nimt insgesamt leicht zu. OpenSubtitle startet bei wiki emb bei $16.75\%$ und steigt auf $20.45\%$, mit twitter emb bei $13.82\%$ und steigt auf $16.28\%$. Reddit wiki emb startet bei $33.69\%$ und endet bei $35.96\%$, twitter emb $30.84\%$ und endet bei $28.68\%$ Der Score von Reddit ist insgesamt höher als derjenige von OpenSubtitle. (ich hab keine Ahnung wieso). Auffällig ist, dass Reddit bei den twitter Embeddings mit zunehmendem Training schlechter wird. Die Ursache könnte vielschichtig sein, einerseits könnte sich das training in einem lokalen minimum befinden, solche Knicks sehen wir bei allen Kurve. Zusätzlich sind die Resultate aber auch mit vorsicht zu geniessen, da wir nicht genau wissen, wie gut die Embeddings zu unseren Anforderungen passen.

\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/s2v_wiki_cosine_similarity.png}
	\centering
	\small
	\text{Evaluation with the wiki model.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/s2v_twitter_cosine_similarity.png}
	\centering
	\small
	\text{Evaluation with the twitter model.}
	\endminipage\hfill
	\caption{Results of the evaluation with Sent2Vec on the outputs of the OpenSubtitles models using the pretrained models found on the Sent2Vec GitHub page.}
	\label{results:sent2vec:opensubtitles:results}
\end{figure}

\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/s2v_wiki_cosine_similarity.png}
	\centering
	\small
	\text{Evaluation with the wiki model.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/s2v_twitter_cosine_similarity.png}
	\centering
	\small
	\text{Evaluation with the twitter model.}
	\endminipage\hfill
	\caption{Results of the evaluation with Sent2Vec on the outputs of the Reddit models using the pretrained models found on the Sent2Vec GitHub page.}
	\label{results:sent2vec:reddit:results}
\end{figure}
\footnotetext{https://github.com/epfml/sent2vec}

\paragraph{Generic answer sentences}.. Eine mögliche Ursache für die schlechten Ergebnisse wären generische Antwortsätze. Diese könnten die Metriken in der Masse so stark beeinflussen, dass vielversprechende Ergebnisse nicht ins Gewicht fallen. Deshalb zählten wir die Anzahl identischer Sätze, Resultate in (see Tables~\ref{results:test_performance:opensubtitles_sample_outputs} and~\ref{results:test_performance:reddit_sample_outputs}). ersichtlich. Anhand der Anzahl gleicher Sätze (Vielleicht können wir proentual angeben, wieviel die top10 sätze ausmachen), scheint es tatsächlich so, dass es generische Antworten gibt. Wir möchten noch einmal mit Hilfe  Sent2Vec scores berechnen, jedoch ohne die Top 1/5/10 häufigsten Antworten. Die Resultate sind dem ursprünglichen so ähnlich, dass wir hier auf eine Visualisierung verzichten. Vielleicht ist dieser Ansatz erfolgreich, wenn man noch mehr generische Antworten aus der Berechnung entfernt. Dies könnte genutzt werden, um in diesem Subsample dann einen Lernfortschritt zu erkennen. Dies ist ja unser ursprüngliches Problem, wir sehen Beispiele, welche besser werden, aber wir können dies nicht in Zahlen darstellen.

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Sentence & Frequency\\ \midrule
			\texttt{i m not gon na let you go} & 41853\\
			\texttt{i m not sure i can trust you} & 21263\\
			\texttt{i m not gon na say anything} & 9163\\
			\texttt{i m not gon na let that happen} & 7426\\
			\texttt{i m sorry} & 7235\\
			\texttt{you re not gon na believe this} & 7068\\
			\texttt{you re not gon na believe me} & 6878\\
			\texttt{i m not gon na hurt} you & 4829\\
			\texttt{i m not a fan} & 4468\\
			\texttt{i m not sure} & 4215\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Top 10 most generated sentences with respective occurence frequencies when using the last OpenSubtitles model on the test dataset.}
	\label{results:test_performance:opensubtitles_sample_outputs}
\end{table}\todo{Totale Anzahl sätze angeben? (können wir ja berechnen?)}

\begin{table}[H]
	\centering
	\ra{1.3}
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Sentence & Frequency\\ \midrule
			\texttt{i m not sure if i m being sarcastic or not .} & 17486\\
			\texttt{i think it s a bit of a stretch .} & 13058\\
			\texttt{i m not sure if you re being sarcastic or not .} & 11647\\
			\texttt{i m not sure if i m a <unknown> or not .} & 8307\\
			\texttt{i m not sure if you re joking or not .} & 7932\\
			\texttt{i was thinking the same thing .} & 7579\\
			\texttt{<unknown>} & 6210\\
			\texttt{i m not sure if i m going to watch this or not .} & 4257\\
			\specialcell{\texttt{i m not sure if i m a fan of the show , but i m}\\\texttt{pretty sure that s a <unknown> .}} & 3232\\
			\texttt{i m not sure if i m going to watch it or not .} & 3079\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Top 10 most generated sentences with respective occurence frequencies when using the last Reddit model on the test dataset.}
	\label{results:test_performance:reddit_sample_outputs}
\end{table}\todo{Totale Anzahl sätze angeben? (können wir ja berechnen?)}

\paragraph{So the Performance...} ist ernüchternd. Die loss Werte auf dem Testset implizieren ein nicht vorhandenes Lernverhalten, welches wir aber mit einzelnen Beispielen widerlegt haben. Wir versuchten anschliessend mit Hilfe Sent2vec einen den Lernfortschritt mit Hilfe der generierter embeddings und der cos distanz sichtbar zu machen. Ohne Erfolg, auch das entfernen der häufigsten Sätze und somit eine Überprüfung von Subsamples, konnte den anhand Beispielen gezeigten Lernfortschritt, nicht stützen. Aufgrund dieser Erkenntnisse möchten wir genauer analysieren, wann welche Elemente des Sprachmodells aus den Trainingsdaten übernommen wurde.

\section{Influence of training data on output}
- Möchten herausfinden, wie sich das Sprachmodell auf n-Gramm Stufe entwickelt. Dies machen wir zuerst auf Stufe bi-Gramm und anschliessend auf den uni-grammen.
- Herausfinden, inwiefern die Struktur der Testdaten übernommen werden
	- Werden die n-gramme mit deren Häufigkeit übernommen?
	- Oder wird nur die Verteilung der häufigsten n-gramme übernommen, nicht aber n-gramme selber.
Anschliessend möchten wir herausfinden, wie unser Modell die Semantik lernt. Wir werden dies mit ähnlichen Sätzen und einer PCA auf 2D durchführen.
Und finaly möchten wir mit einzelnen Beispielen herausfinden, wie der attention mechanismus funktioniert und dessen entscheidung im Kontext sinnvoll ist.
\subsection{Bi-Gramm}
In den Grafiken \ref{results:ngram:distributions:opensubtitles} und \ref{results:ngram:distributions:reddit} wird dargestellt (erklären, was genau man sieht, muss genau sein, da nicht ganz einfach.)
\paragraph{Die bi-gramme und deren Wahrscheinlichkeiten werden übernommen.}
\todo{grafiken interpretieren}
\paragraph{The distribution is right-handed} of the words... Für beide Modelle gilt: Starten gleichverteilter, als expected, nähern sich dann dem expected Werten an. Dieser Virgang scheint von links nacht rechts stattzufinden. Das scheint insofern nachvollziehbar, dass er schnell merkt, welche bi-gramme nicht oft vorkommen und entsprechend diese fast nie verwendet.Jedoch herauszufinden, welches der top n n-gramme das Richtige ist, ist ein deutlich schwierigerer Lernprozess. Wir sehen im groben zumindest eine Annäherung der Verteilung. Wünschenswert wäre natürlich hier eine ähnliche Verteilung wie expected, wobei vor allem die weniger häufigen Bigramme für eine vielfältige Sprache stehen. Dies würde sich ganz generell als mass für die Sprachvielfalt eigenen, indem man z.B. die umgekehrte Wahrscheinlichkeit als Gewicht Einfliessen lässt.
\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_500000.pdf}
	\centering
	\small
	\text{Distribution at training step 500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_1000000.pdf}
	\centering
	\small
	\text{Distribution at training step 1'000'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_1500000.pdf}
	\centering
	\small
	\text{Distribution at training step 1'500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_2000000.pdf}
	\centering
	\small
	\text{Distribution at training step 2'000'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_2500000.pdf}
	\centering
	\small
	\text{Distribution at training step 2'500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/opensubtitles_not_reversed/bigram_distribution_comparison_step_3000000.pdf}
	\centering
	\small
	\text{Distribution at training step 3'000'000.}
	\endminipage\hfill
	\caption{Distributions of the top 100 opensubtitles bigrams in the generated answers when using the test set over multiple time steps of the training.}
	\label{results:ngram:distributions:opensubtitles}
\end{figure}

\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_500000.pdf}
	\centering
	\small
	\text{Distribution at training step 500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_1000000.pdf}
	\centering
	\small
	\text{Distribution at training step 1'000'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_1500000.pdf}
	\centering
	\small
	\text{Distribution at training step 1'500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_2000000.pdf}
	\centering
	\small
	\text{Distribution at training step 2'000'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_2500000.pdf}
	\centering
	\small
	\text{Distribution at training step 2'500'000.}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{img/plots/reddit/bigram_distribution_comparison_step_3000000.pdf}
	\centering
	\small
	\text{Distribution at training step 3'000'000.}
	\endminipage\hfill
	\caption{Distributions of the top 100 reddit bigrams in the generated answers when using the test set over multiple time steps of the training.}
	\label{results:ngram:distributions:reddit}
\end{figure}

\subsection{Uni-gramm/Wörter}
\todo{grafiken erstellen und referenzieren und beschreiben}
\paragraph{Die Uni-gramm und deren Wahrscheinlichkeiten werden übernommen.}
\todo{grafiken interpretieren}
\paragraph{The distribution is}
\todo{grafiken interpretieren}

\subsection{Does the Model have an understanding of natural language sentiment?}

\paragraph{Thought Vectors for Input Sequences}After the encoder has processed the whole input sequences, it pass it forward to the decoder to construct the output sequence (see Chapter~\ref{fundamentals:seq2seq}). It is the only direct connection the encoder and decoder have in such a model, which means, that the encoder has to ``encode'' all the information into this thought vector before passing it to the decoder. The thought vector hence represents an embedding of the input sequence in an $n$ dimensional vector space, where $n$ stands for the size of the thought vector. To analyze this embeddings, we collected them for 15 different sample sentences and projected them via PCA into two dimensional space, the results of this projection can be see in Figure~\ref{results:thougth_vectors:embeddings:opensubtitles} and~\ref{results:thougth_vectors:embeddings:reddit} below.

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm]{img/opensubtitles_thought_vector_embeddings.png}
	\caption{The projected thought vectors for 15 different sentences when using the OpenSubtitles model. PCA was used for the projection.}
	\label{results:thougth_vectors:embeddings:opensubtitles}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=16cm]{img/reddit_thought_vector_embeddings.png}
	\caption{The projected thought vectors for 15 different sentences when using the Reddit model. PCA was used for the projection.}
	\label{results:thougth_vectors:embeddings:reddit}
\end{figure}

Both of the models seem to have no problems understanding clear, direct sentences where the intent is clear (e.g. ``I have no interest in your opinion on movies, your taste is bad!''). This can be seen because similar sentences are clustered together in the projected space. However, when it comes to curses and questions regarding the gender, the OpenSubtitles model starts to struggle, which can be seen by taking a look at the respective points in the projected space. For example, the questions regarding the gender or the curses are scattered throughout the space, even though they should have been embedded closely to each other. The Reddit model seems to have less problems with this, as the embeddings for these sentences are quite close together. But what is interesting to see is that the Reddit model embeds the sentences with curses close to the sentences regarding the gender.\todo{write more}



\section{What Language Model do the Models produce?}
\blindtext

\section{Does the Model have an understanding of natural language?}
\blindtext

\section{How can we fix the detected problems?}
\blindtext



\section{Generated outputs over time}
\blindtext

\section{Comparison with CleverBot and ``Neural Conversational Model''}
\blindtext


\section{Reverse Input Feeding}
\blindtext