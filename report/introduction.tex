\chapter{Introduction}
The development of dialog systems or chatbots which are capable of passing the Turing test~\cite{Turing:1950} is a long lived dream among the research community. Since the publication of the Turing test, a lot of efforts have been taken to achieve this goal, but none was so far able to\footnote{http://isturingtestpassed.github.io/}, even though there were some serious contenders. The research area has gained a lot of attention in the past few years due to the technical advancements occurred over the last two decades. One of the big leaps in technology was the inception of machine learning, especially deep learning. The theoretical fundamentals have been available for a long time, but were not applicable due to the lack of computational power and amount of processed data required by such systems. However, changed in the past few years where deep learning had a huge influence on various different fields, for tasks such as image recognition, natural language processing or anomaly detection. They have proven that it is indeed viable to use this technology nowadays because of the exponential growth in available data and computing power.

In this thesis, we will try to develop a conversational dialog system comparable to the one published in the paper \emph{Neural Conversational Model} by Oriol Vinyals and Quoc V. Le~\cite{Vinyals:2015}. Our main motivation for building such a system is twofold: First, their system is an end-to-end system, meaning that the system as a whole can be designed and trained without the obstacle of managing several different subcomponents necessary for performing such a task (i.e. language understanding, interpretation and generation). Our second thought lies in the sheer enthusiasm about the fact that the system presented in the mentioned paper, despite its simplicity, was able to learn to respond meaningful to important questions, such as:

\begin{center}	
	\textbf{Human}: what is the purpose of life ?\\
	\textbf{Machine}: to serve the greater good .
\end{center}

We were blown away when we first saw the machine answering in such a decisive and comprehensible way we would never have imagined possible.

Our goal is to build a system comparable to the system in the paper, but on a much smaller scale. Our aim is to implement a system which is capable of being trained on a single GPU, instead of requiring an entire server with all its RAM and CPUs. For this reason, we investigate if such a model can also be used if scaled-down significantly, namely half of the original size. We then use this scaled down version of this model to train it on two separate datasets: One on the OpenSubtitles~\cite{Lison:2016} and the other on the Reddit Comment Dataset\footnote{https://archive.org/details/2015\_reddit\_comments\_corpus}, specifically using the comments from the subreddits \emph{movies}, \emph{television} and \emph{films}. We have chosen the first dataset in order to evaluate if a smaller model can compete with the results from the mentioned paper. The main idea behind choosing the second dataset is that we want the resulting dialog system to be able to talk about movie-related topics.

The trained models are then analyzed in different dimensions. First, we are exploring the issue of performance metrics and try if a newly proposed metric based on the Sent2Vec~\cite{Pgj:2017} library is sufficient for evaluating semantic similarity in such a setup. We then analyze, how the model learns to talk and try to evaluate which aspects are important for doing so by analyzing the produced language models and compare these models to the language models found in the mentioned datasets. A comparison between our models and the model found in the paper and the \emph{CleverBot}\footnote{http://www.cleverbot.com/} then follows. Finally, our goal is to analyze if beam-search helps to diversify the answers and if and how the soft-attention mechanism~\cite{Bahdanau:2014} can help the models to increase comprehension.