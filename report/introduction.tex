\chapter{Introduction}
The development of dialog systems or chatbots which are capable of passing the Turing test~\cite{Turing:1950} is a long lived dream among the research community. Since then, a lot of efforts have been taken to reach this goal, but none has\footnote{http://isturingtestpassed.github.io/}, even though there were some serious contenders. Nevertheless, this research area has gained a lot of attention in the past few years due to the technical advancements humanity has witnessed in the last two decades. One of the big leaps in technology was the inception of machine learning, especially deep learning. The theoretical fundamentals have been available for a long time, but never been used in practice, because most of the researchers thought that it will never be viable to use such systems due to the lack of computational power and amount of data. However, this is not true anymore as we have seen in the past few years where deep learning had a huge impact on the technical landscape, for tasks such as image recognition, natural language processing or anomaly detection. They have proven that it is indeed viable to use this technology nowadays because of the exponential growth in available data and computing power.

In this thesis, we are going to try to develop a conversational dialog system similar to the one published in the paper \emph{Neural Conversational Model} by Oriol Vinyals and Quoc V. Le~\cite{Vinyals:2015}. Our main motivation for building such a system is twofold: First, their system is an end-to-end system, which means that the system as a whole can be designed and trained without the hesitation of managing several different subcomponents necessary for performing such a task (i.e. language understanding, interpretation and generation). Our second thought lies in the sheer fact, that the presented system, despite its simplicity, was able to learn to respond meaningful to and important questions, such as:

\begin{center}	
	\textbf{Human}: what is the purpose of life ?\\
	\textbf{Machine}: to serve the greater good .
\end{center}

We were blown away when we first saw the machine answering in such a decisive and comprehensible way we would never have imagined.

Our goals is to rebuild the system of the paper but on a much smaller scale. We seek to implement a system which is capable of being trained on a single GPU, instead of a whole server with all its RAM and CPUs. For this reason, we investigate if such a model can also be used if scaled down by a large factor, namely half of the original size. We then use this scaled down version of this model to train it on two separate datasets: One on the OpenSubtitles~\cite{Lison:2016} and the other on the Reddit Comment Dataset\footnote{https://archive.org/details/2015\_reddit\_comments\_corpus}, specifically using the comments from the subreddits \emph{movies}, \emph{television} and \emph{films}. We have chosen the first dataset to be able to evaluate if a smaller model can compete with the results from the mentioned paper. The main idea behind choosing the second dataset is that we want to be able to communicate with the resulting dialog system and talk about movie-related topics.

The trained models are then analyzed with regard of several different aspects. First, we are trying to dive into the issue of performance metrics and try if a newly proposed metric based on the Sent2Vec~\cite{Pgj:2017} library are sufficient for evaluating semantic similarity. We then analyze, how the model learns to talk and try to find out which aspects are important for doing so by analyzing the produced language models and compare them to the language models found in the mentioned datasets. A comparison between our models and the model found in the paper and the \emph{CleverBot}\footnote{http://www.cleverbot.com/} then follows. At the end, our goals is to analyze if beam-search helps to diversify the answers and if and how the soft-attention mechanism~\cite{Bahdanau:2014} can help the models to comprehend.