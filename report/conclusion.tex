\chapter{Conclusion}

We started this thesis with the goal to build an end-to-end dialog system using the architecture and technologies from the ``Neural Conversational Model'' paper of Vinyals and Le~\cite{Vinyals:2015}. Our principle idea was to build such a system, but on a much smaller scale. The goal of this reduction was to fit the resulting system onto a single GPU, instead of using the resources of an entire server as they did in the mentioned paper.

The implementation of our system was challenging, but it allowed us to gain insight on how such systems are implemented on a much lower level and taught us many different things, including how a computational graph works, the inner mechanisms of sequence-to-sequence models. We even had the chance to implement an in-graph beam-search decoder.

Due to difficulties with several different APIs of the TensorFlow framework, it took much longer than expected until we could complete our first, running version of the system. But, after all the initial trouble, more than half of the semester has already passed by then, we could finally start the training of our models on the two datasets. We have decided to use this two datasets, namely the Reddit comment and OpenSubtitles dataset, for two reasons: First, our initial goals was to build a system which could talk about movie related topics. The second reason was, that these two datasets represent two different types of language, spoken and written. Handling and preprocessing both of these datasets was rather challenging, because both of them are huge datasets; in fact bigger than anything we have handled before. The Reddit dataset also imposed another difficulty because we had to reconstruct the original discussions by using a tree structure.

After we have obtained both datasets and a functional version of the system we started to determine the maximum possible size of the model still fitting on a single GPU. It took some time to define all required hyperparameters as they determine the final memory consumption of the model.

When we found the maximum achievable size for the models, we proceeded with the training. To track the development of the models over the course of the training period, we created snapshots of the models at six distinct points in time. After more than three weeks of training time, we had gathered the our six snapshots and could start the analysis phase.

At first glance, everything looked fine and the loss and perplexity values on the training datasets indicated was overall successful. There were differences in the speed and way how the models learned over the course of the training. Obvious was the high variance in the metrics for OpenSubtitles which we had not observed with the Reddit model. We presume that this was related to the differently structured datasets, as the Reddit dataset has a clear structure implied by the nature of the raw data. This was in clear contrast to the OpenSubtitles datasets, were no information about turn-taking or structure of the conversations is available.

We then started the evaluation of our models with the respective test datasets, based on the mentioned cross-entropy loss and perplexity metrics. Additionally, we also explored the usability of a new metric based on the recently published Sent2Vec library. At first, we were negatively surprised by the results and considered the entire experiment has failed. However, we subsequently found several reasons for why the models did not show the expected performance. Our first finding was, that the cross-entropy loss function and the related perplexity metric might not be the most suitable metrics for measuring the performance of a conversational dialog system, as the range of possible appropriate responses is huge. We were aware of this problem from the beginning. Because of this, we also considered a new, alternative metric based on the Sent2Vec library. However, the results of the performance assessment of the models with this metric did also not provide results as expected. We assume, as with the other metrics, the problem lies in the large proportion of generic responses. One interesting fact we derived from the results was, that the Reddit model performed twice as good as the OpenSubtitles model in this evaluation. We concluded this to be related to the differences between spoken and written language, which are both present in our datasets. Overall, the results of the metrics did, in our opinion, not reflect the effective performance of the models because we had already talked to them and considered the model's responses to be better than the metrics implied. Therefore, we started to investigate into the language models to find the reason for the poor results on the metrics.

Our first attempt to find a reason for the metric's poor results was to look at the generated responses. We quickly noticed the lack variety in the responses. Instead, our models often generated generic responses, such as ``i don t know'' and ``i m not sure''. We started to investigate into how our models adopted the language models from the training datasets. We did this by looking at the development of uni- and bi-gram distributions over the course of the training. This revealed, that the learning process of the OpenSubtitles model differed from the learning process of the Reddit model. The OpenSubtitles model exhibited much more variance, which we attributed to the much more noisey dataset, compared to the Reddit dataset. This analysis also revealed, that it is not recommended to use smaller dataset and iterate over it multiple times, as we did with the Reddit model. The result was, that the later snapshots of the Reddit model showed a much poorer overall performance on metrics. Similar effects were observed in the subjective evaluation, where we talked to the model directly.

We still believed that the generic responses could be explained somehow. This is why we conducted another analysis were we analyzed the usage frequencies of the top 10 most used uni-, bi-grams and sentences in the outputs when using the test datasets. Our intention with this analysis was to see how the language variety evolves over time. The results showed that the amount of generic responses decreases over time. We could also confirm our subjective impression that the models have a tendency to improve with extended training time, especially the OpenSubtitles model. For the Reddit model, we could validate through this analysis that the language variety and hence, the performance of the model is at its peak when using the 1.5M or 2.0M snapshots. Finally, we did not find a conclusive answer to the question what the reasons for the generic answers are. However, we could show, that this problem reduces as the models have more time to learn.

After all the analysis, we started to compare our models to two others, namely the CleverBot and the results from the paper mentioned at the beginning of this chapter. Our models delivered comparable performance as the CleverBot and the results from the paper, as long as the utterances did not involve complicated or indistinct sentences. In general, the results from the comparison our models to the result from the paper were disillusioning, because most of the utterances we used from the paper yielded poor results on our side. However, this was not a surprise, as we already knew that the samples from the paper included a lot of complicated topics, such as morality and philosophy. We concluded that the driving factor that the responses by our models were not as expressive and thoughtful as the ones in the paper are the shorter training time and the limited model size.

As the last analysis, we took a quick look into the beam-search implementation, the soft-attention mechanism and the clustering of thought vectors. This revealed, that the results from the beam-search decoder are often better then when using the greedy decoder. The analysis of the soft-attention mechanism and its impact on the generation of responses by our models showed, that it limited usability of the approach in the context of conversational models; as already noticed by others. The clustering of generated thought vectors however showed, that our models indeed had an understanding of language, as most of the sentences with comparable meaning were clustered tightly.

In summary, we are satisfied with the outcome of this thesis, even though the results were not as good as initially expected. It provided us the opportunity to explore into a complete new and previously unknown topic in the machine learning domain. We gained knowledge and insight into many new theories, research areas and architectures, while simultaneously allowing us to implement a system to solve a really interesting challenge. We learned much about the most recent innovations in the field of dialog systems. We would have preferred more time for the elaboration of this thesis, as this would have allowed us to extend the time for the training of the model and even deeper investigation of certain aspects.

\chapter{Future Work}

To finish our theses, we would like to give several leads how the developed system and models could be extended in a future work:

\begin{itemize}
  \item The first, obvious change we would have liked to make is to extend the time period used for training. Such models need a lot of time to be trained and as our analysis and also our impression has shown, it would be advisable to train the mentioned models for a longer period than only three weeks. We cannot say anything about what the most appropriate time period would be. This is especially true for the OpenSubtitles model as we were only able to process a little more than half of the training dataset. For the Reddit model, the size of the dataset would obviously have to be extended in order that such a long training makes sense.
  
  \item The second, maybe obvious point, is to grow the size of the model itself. This is not easily possible, as we already used the largest size possible so that the model still fit on a single GPU. However, the implementation of the models could be extended to support multi-GPU use-cases. This would require a major redesign of the implementation of the models, as the computional graph would have to be split across the multiple GPUs. This is possible in TensorFlow, via the \texttt{tf.device} API\footnote{https://www.tensorflow.org/api\_docs/python/tf/device}, which allows for running the computations across multiple different devices (i.e. GPUs, CPUs). Such a split was already done by other people, for example in one of the original sequence-to-sequence learning papers by Sutskever et al \cite{Sutskever:2014}.

  \item We had to fix the size of our computational graph to use 30 time steps for the encoder and decoder each. This is due to the fact, that we had to use an old TensorFlow API (see Chapter~\ref{software_system:development_history}) because we were running into huge problems when first developing our system. Another, also obvious change, would be to use the dynamic RNN API\footnote{https://www.tensorflow.org/api\_docs/python/tf/nn/dynamic\_rnn} from the most recent TensorFlow version to implement the models.
  
  \item Another point is to try different RNN cells instead of LSTM. For example, Gate Recurrent Units \cite{Chung:2014} or even Convolutional LSTM \cite{Xingjian:2015} cells could be used. Another idea is to try other extensions to LSTM cells, for example by using peephole connections.

  \item We located the problem, that our model often generates generic responses. This could be tackled by using dedicated loss function which trains the models with an incentive on language variety, such as the loss function found in the paper by Li et al. \cite{Li:2016}.

  \item One of the main issues we had were the metrics to asses the performance of such conversational models. We are currently not aware of any other popular metrics used in the field, besides the cross-entropy loss and the perplexity. However, it would be certainly possible and a really interesting research topic to develop new metrics, such as we tried with our Sent2Vec metric.

  \item Another interesting point would be the possibility to formulate the sequence-to-sequence learning as a beam-search optimization problem as Wiseman et al. \cite{Wiseman:2016} did.

  \item A point which we have not covered at all in this thesis is context between the utterances within a conversation. This is a hard problem, as the models would have to store the context somehow between the utterances. Two ways on how to head into that direction are shown in the papers by Yao et al. \cite{Yao:2015} and Serban et al. \cite{Serban:2015}.
\end{itemize}
