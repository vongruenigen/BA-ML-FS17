\chapter{Conclusion}
Wir starteten unsere Arbeit mit der Idee eines Nachbaus des paper of Vinyals and Le~\cite{Vinyals:2015}? Dazu mussten erst einmal herausfinden, welche Modell \todo{Dirk, schreibst du den ersten Teil, kurzer Beschreib bis zum definitiven modell. Das würde bedeuten Projektion und etc wäre auch hier}
Nachdem wir nun ein lauffähiges Modell hatten, ging es darum die Daten in eine richtige Form zu bringen. Zusätzlich zum OpenSubtitle welcher gesprochene Sprache repräsentiert, verwendeten wir Reddit welche geschrieben Sprache ist. \todo{das ist doch die erklären fürs noisy? und die schlechteren Scorse bei Sent2Vek und eben auch der Zackenkurve?} Bei den Reddit Daten mussten wir wie im Kapitel \ref{chapter:data} beschrieben zusätzlich wieder die Struktur der ursprünglichen Konversation herstellen. Wir mussten ebenfalls analysieren, inwiefern der beste Trade-of zwischen Vokabulargrösse und grösse des Modells liegt.
Nachdem wir dies eruiert haben, starteten wir das Training wie in \ref{software_system} beschrieben und speicherten jeweils alle 0.5M trainingsdaten einen Snapshot. Dies mit der Idee später den Verlauf des Trainings genauer analysieren zu können. Nach ca 3 Wochen trainingszeit werteten wir die Loss/Perplex Werte auf dem trainings und validierungsset aus. Die Kurven zeigten ein unterschiedliches Lernverhalten, konkret OpenSubtitles hohe Varianz, Reddit dips und plateaus?. Waren aber tendenziell sinken. Die gleiche Auswertung auf den Testsets resultierte in einem anderem Ergebnis. Die Loss/Perplex. bei OpenSubtitles ist nicht interpretierbar und diejenige von Reddit steigt kontinuierlich ganz leicht . Dies stand in einem Widerspruch zu unserem Subjektiven Empfinden, dass replies mit fortlaufendem Training besser werden.
Wir wussten, dass die Loss/Perplex als Metrik für Sprache, im besonderen für ein Dialogsystem, beim welchem es keine eindeutig richtige Antwort gibt, nicht über alle zweifel erhaben ist. Dehsalb testen wir unsere Resultate mit Sent2vec. Die Ergebnisse dabei waren ebenfalls durchzogen. Auffallend hierbei war, dass die (cos Distanz Metrik) bei Reddit etwa doppelt so gross war, wie bei OpenSubtitle. Eine mögliche Ursache, könnte der unterschied gesprochene/geschrieben Sprache sein.

Abgesehen davon war das erneute negative Resultat ein Problem, unseren subjektiven Eindruck zu bestätigen. Wir merkten beim interagieren mit unsere Modellen auch, dass es eine Art generische Antwortsätze gibt. Sätze die eben sehr oft als replie erscheinen, vermutlich weil sie eben für viele Inputs eine gute loss besitzen. Deshalb versuchten wir genauer zu analysieren, wie unsere Modelle das Sprachmodell aus den Trainingsdaten übernommen haben und auf welche Stufe. Dies analysierten wir im Kapitel \ref{results:chapter} auf Stufe uni-gram, bi-gram and sentences. Wir bemerkten dabei, dass die Verteilung auf Stufe bi-gram und uni-gram ähnlich dem trainingsset übernommen wurde. Besonders die Verteilung der häufigsten ca 100? sind relativ ähnlich. Unterschiede gibt es in den weniger häufügen n-grams (satz kan auch weggelassen werden). Dabei gab es pro Corpus 2 Auffälligkeiten. OpenSubtitle wechselte immer wieder zwischen einer ziemlich ähnlichen Verteilung zu einer rechtslastigen Verteilung\todo{müssen wir vielleicht noch erwähnen, dass rechtslastig eigentlich richtig ist, aber eben nicht zuu rechtslastig. also eben nur so rechtslastig wie expected}. Reddit scheint bis zum snapshot 2.0M sich stetig anzunähern und wird ab dann nur noch rechtslastiger. Diese 2 Besonderheiten wollten wir genauer untersuchen, weshalb wir die top 10 Häufigsten uni-gram, bi-gram, und deren Prozentualer Anteil an der Gesamtmenge verwendeter uni-gram, bi-gram, sentences. Mit dieser Vorgehensweise und der anschliessenden Visualisierung konnten wir aufzeigen, dass der Anteil der Top 10 pro Kategorie tendenziell sinkt. Wobei die beiden Corpora wieder unterschiedliche Resultate ergaben. Bei OpenSubtitle sahen wir eine unstetige (kann man das so nehmen? mathematisch glab nicht korrekt) Enwicklung. Insgesamt wurden die lokalen maximas jedoch kleiner. Dies könnte wie im Paper \cite{bibid} beschrieben mit dem noisy zu tun haben, was wir grösstenteils auf die gesprochene Sprache zurückführen. Reddit zeigt ein stetige reduzierung bis zu dem Punkt, als wir das 2. Mal mit den gleichen Daten trainierten. Wir konnten damit unser positves empfinden während dem Testen mit Hilfe des Anteils der top 10 kategorieren darstellen.

Anschliessend verglichen wir die einiges ausgewählte Resultate mit Cleverbot und Paper. Dabei zeigte sich, dass unser System durchaus mithalten konnte. Wobei bei anspruchsvollen Interaktion wir im Gegensatz zum Paper viel schlechtere Antworten generierten. Die Ursache dafür sehen wir in 2 Punkten, die Dauer des Trainings und die grösse des Modells. Es war uns leider nicht möglich, mit mehr als 2048 zu trainieren. Trotzdem sehen wir die Arbeit insgesamt als Erfolg. Es war uns möglich, das System nachzubauen und zu trainieren. Auch gab es durchaus gute Antworten und wir konnten ebenfalls das Sprachmodell in einem begrenzten Rahmen analysieren.
Beam-seach?
Attention?


Wir testeten anschliessend der analysierten wir den Verlauf des Trainings mit Hilfe der Loss/Perplex. die ersten Eindrücke nach 3 wöchigem Training waren alle positiv. Daraufhin
\chapter{Future Work}
\blindtext