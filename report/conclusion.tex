\chapter{Conclusion}

We started this thesis with the goal to build an end-to-end dialog system using the architecture and technologies from the ``Neural Conversational Model'' paper of Vinyals and Le~\cite{Vinyals:2015}. Our main idea was to build such a system, but on a much smaller scale. The goal of this reduction was to fit the resulting system onto a single GPU, instead of using the ressources of a whole server as they did in the paper.

The implementation of our system was a big challenge, but it allowed to us to learn more about the details on how such systems are implemented on a much lower level and taught us many different things, including how a computational graph works, the inner workings of sequence-to-sequence models and we even had the chance implement an in-graph beam-search decoder at the end.

Due to difficulties with several different APIs from the TensorFlow framework, it took a much longer time than expected until we had our first, running version of the system. But, after all the trouble, after more than half of the semester has already passed, we could finally start the training of our models on the two datasets, namely the Reddit comment and the OpenSubtitles dataset. We have decided to use this two datasets for two reasons: First, our initial goals was build a system with which we could talk about movie related topics. The second reason was, that these two datasets represent two different types of language, spoken and written. Handling and preprocessing both of these datasets was pretty big challenge, because both of them are huge, bigger than anything we have handled before. The Reddit dataset also imposed another difficulty because we had to reconstruct the original discussions by using tree structure.

After we have obtained both datasets and a working version of the system we started to evaluate how big in terms of size we could make our models ...

When we found the maximum achievable size with the restriction of using only a single GPU per model, we started their training. To keep track of the development of the models throughout the training, we took snapshots of them at six distinct points in time. After more than three weeks of training time, we had our six snapshots and could start by analyzing the results.

At first glance, everything looked fine and the loss and perplexity values on the training datasets suggested that the training went fine overall. There were differences in the speed and way how the models learned throughout the time span of the training, especially apparent was the high variance in the metrics for OpenSubtitles which we had not seen with the Reddit model. We concluded that this had to do with the differently structured datasets, as the Reddit dataset has a clear structure implied by the structure of the conversations on Reddit. This was different with the OpenSubtitles dataset, as we had no information about turn-taking or structure of the conversations.

We then went on and started evaluating our models with the respective test datasets with the mentioned loss and perplexity metrics. Additionally, we also tried a new metric based on the recently published Sent2Vec library. At first, we were shocked by the results and thought that the whole experiment has failed. However, we could then find several reasons for why the models did not show the performance we first expected. Our first finding was, that the cross-entropy loss function and the related perplexity metrics might not be the most suitable metrics for measuring the performane of a conversational dialog system, as the range of possible responses which are appropriate is extremely huge. We were aware of this problem from the beginning, why we put some thoughts into a new metric based on the Sent2Vec library. However, the results of assessing the performance of the models with this metric did also not bring the good results we wished to achieve. One interesting fact we could extract from the results was, that the Reddit model was twice as good as the OpenSubtitles model in this evaluation. We concluded that this has to do with the differences between spoken and written language, which are both present in our datasets. Overall, the results did in our opinion not reflect the performance of the models because we had already talked to them and thought that the results were not that bad. We then started to investigate into the language models to find the reason for the poor results.





Wir starteten unsere Arbeit mit der Idee eines Nachbaus des paper of Vinyals and Le~\cite{Vinyals:2015}? Dazu mussten erst einmal herausfinden, welche Modell \todo{Dirk, schreibst du den ersten Teil, kurzer Beschreib bis zum definitiven modell. Das würde bedeuten Projektion und etc wäre auch hier}

Nachdem wir nun ein lauffähiges Modell hatten, ging es darum die Daten in eine richtige Form zu bringen. Zusätzlich zum OpenSubtitle welcher gesprochene Sprache repräsentiert, verwendeten wir Reddit welche geschrieben Sprache ist. \todo{das ist doch die erklären fürs noisy? und die schlechteren Scorse bei Sent2Vek und eben auch der Zackenkurve?} Bei den Reddit Daten mussten wir wie im Kapitel \ref{chapter:data} beschrieben zusätzlich wieder die Struktur der ursprünglichen Konversation herstellen. Wir mussten ebenfalls analysieren, inwiefern der beste Trade-of zwischen Vokabulargrösse und grösse des Modells liegt.
Nachdem wir dies eruiert haben, starteten wir das Training wie in \ref{software_system} beschrieben und speicherten jeweils alle 0.5M trainingsdaten einen Snapshot. Dies mit der Idee später den Verlauf des Trainings genauer analysieren zu können. Nach ca 3 Wochen trainingszeit werteten wir die Loss/Perplex Werte auf dem trainings und validierungsset aus. Die Kurven zeigten ein unterschiedliches Lernverhalten, konkret OpenSubtitles hohe Varianz, Reddit dips und plateaus?. Waren aber tendenziell sinken. Die gleiche Auswertung auf den Testsets resultierte in einem anderem Ergebnis. Die Loss/Perplex. bei OpenSubtitles ist nicht interpretierbar und diejenige von Reddit steigt kontinuierlich ganz leicht . Dies stand in einem Widerspruch zu unserem Subjektiven Empfinden, dass replies mit fortlaufendem Training besser werden.

Wir wussten, dass die Loss/Perplex als Metrik für Sprache, im besonderen für ein Dialogsystem, beim welchem es keine eindeutig richtige Antwort gibt, nicht über alle zweifel erhaben ist. Dehsalb testen wir unsere Resultate mit Sent2vec. Die Ergebnisse dabei waren ebenfalls durchzogen. Auffallend hierbei war, dass die (cos Distanz Metrik) bei Reddit etwa doppelt so gross war, wie bei OpenSubtitle. Eine mögliche Ursache, könnte der unterschied gesprochene/geschrieben Sprache sein.

Abgesehen davon war das erneute negative Resultat ein Problem, unseren subjektiven Eindruck zu bestätigen. Wir merkten beim interagieren mit unsere Modellen auch, dass es eine Art generische Antwortsätze gibt. Sätze die eben sehr oft als replie erscheinen, vermutlich weil sie eben für viele Inputs eine gute loss besitzen. Deshalb versuchten wir genauer zu analysieren, wie unsere Modelle das Sprachmodell aus den Trainingsdaten übernommen haben und auf welche Stufe. Dies analysierten wir im Kapitel \ref{results:chapter} auf Stufe uni-gram, bi-gram and sentences. Wir bemerkten dabei, dass die Verteilung auf Stufe bi-gram und uni-gram ähnlich dem trainingsset übernommen wurde. Besonders die Verteilung der häufigsten ca 100? sind relativ ähnlich. Unterschiede gibt es in den weniger häufügen n-grams (satz kan auch weggelassen werden). Dabei gab es pro Corpus 2 Auffälligkeiten. OpenSubtitle wechselte immer wieder zwischen einer ziemlich ähnlichen Verteilung zu einer rechtslastigen Verteilung\todo{müssen wir vielleicht noch erwähnen, dass rechtslastig eigentlich richtig ist, aber eben nicht zuu rechtslastig. also eben nur so rechtslastig wie expected}. Reddit scheint bis zum snapshot 2.0M sich stetig anzunähern und wird ab dann nur noch rechtslastiger. Diese 2 Besonderheiten wollten wir genauer untersuchen, weshalb wir die top 10 Häufigsten uni-gram, bi-gram, und deren Prozentualer Anteil an der Gesamtmenge verwendeter uni-gram, bi-gram, sentences. Mit dieser Vorgehensweise und der anschliessenden Visualisierung konnten wir aufzeigen, dass der Anteil der Top 10 pro Kategorie tendenziell sinkt. Wobei die beiden Corpora wieder unterschiedliche Resultate ergaben. Bei OpenSubtitle sahen wir eine unstetige (kann man das so nehmen? mathematisch glab nicht korrekt) Enwicklung. Insgesamt wurden die lokalen maximas jedoch kleiner. Dies könnte wie im Paper \cite{bibid} beschrieben mit dem noisy zu tun haben, was wir grösstenteils auf die gesprochene Sprache zurückführen. Reddit zeigt ein stetige reduzierung bis zu dem Punkt, als wir das 2. Mal mit den gleichen Daten trainierten. Wir konnten damit unser positves empfinden während dem Testen mit Hilfe des Anteils der top 10 kategorieren darstellen.

Anschliessend verglichen wir die einiges ausgewählte Resultate mit Cleverbot und Paper. Dabei zeigte sich, dass unser System durchaus mithalten konnte. Wobei bei anspruchsvollen Interaktion wir im Gegensatz zum Paper viel schlechtere Antworten generierten. Die Ursache dafür sehen wir in 2 Punkten, die Dauer des Trainings und die grösse des Modells. Es war uns leider nicht möglich, mit mehr als 2048 zu trainieren. Trotzdem sehen wir die Arbeit insgesamt als Erfolg. Es war uns möglich, das System nachzubauen und zu trainieren. Auch gab es durchaus gute Antworten und wir konnten ebenfalls das Sprachmodell in einem begrenzten Rahmen analysieren.
Beam-seach?
Attention?


Wir testeten anschliessend der analysierten wir den Verlauf des Trainings mit Hilfe der Loss/Perplex. die ersten Eindrücke nach 3 wöchigem Training waren alle positiv. Daraufhin
\chapter{Future Work}
Weiter trainieren
grösserer Corpus
granularerer snapshots
Output (grösse hiddenstate) Grösse verdoppeln zum inputlayer? (gelesen irgendwoe, sprachvarianz findet im encoder stat)
Bidirektional?