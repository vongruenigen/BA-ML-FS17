\chapter{Conclusion}

We started this thesis with the goal to build an end-to-end dialog system using the architecture and technologies from the ``Neural Conversational Model'' paper of Vinyals and Le~\cite{Vinyals:2015}. Our main idea was to build such a system, but on a much smaller scale. The goal of this reduction was to fit the resulting system onto a single GPU, instead of using the ressources of a whole server as they did in the paper.

The implementation of our system was a big challenge, but it allowed us to learn more about the details on how such systems are implemented on a much lower level and taught us many different things, including how a computational graph works, the inner workings of sequence-to-sequence models and we even had the chance to implement an in-graph beam-search decoder at the end.

Due to difficulties with several different APIs from the TensorFlow framework, it took a much longer time than expected until we had our first, running version of the system. But, after all the trouble, after more than half of the semester has already passed, we could finally start the training of our models on the two datasets, namely the Reddit comment and the OpenSubtitles dataset. We have decided to use this two datasets for two reasons: First, our initial goals was build a system with which we could talk about movie related topics. The second reason was, that these two datasets represent two different types of language, spoken and written. Handling and preprocessing both of these datasets was pretty big challenge, because both of them are huge, bigger than anything we have handled before. The Reddit dataset also imposed another difficulty because we had to reconstruct the original discussions by using tree structure.

After we have obtained both datasets and a working version of the system we started to evaluate how big in terms of size we could make our models so that they would still fit onto a single GPU. This took some time, as a lot of different hyperparameters are responsible for the final memory consumption.

When we found the maximum achievable size for the models, we started their training. To keep track of the development of the models throughout the training, we took snapshots of them at six distinct points in time. After more than three weeks of training time, we had our six snapshots and could start by analyzing the results.

At first glance, everything looked fine and the loss and perplexity values on the training datasets suggested that the training went fine overall. There were differences in the speed and way how the models learned throughout the time span of the training, especially apparent was the high variance in the metrics for OpenSubtitles which we had not seen with the Reddit model. We concluded that this had to do with the differently structured datasets, as the Reddit dataset has a clear structure implied by the structure of the conversations on Reddit. This was different with the OpenSubtitles dataset, as we had no information about turn-taking or structure of the conversations.

We then went on and started evaluating our models with the respective test datasets with the mentioned loss and perplexity metrics. Additionally, we also tried a new metric based on the recently published Sent2Vec library. At first, we were shocked by the results and thought that the whole experiment has failed. However, we could then find several reasons for why the models did not show the performance we first expected. Our first finding was, that the cross-entropy loss function and the related perplexity metrics might not be the most suitable metrics for measuring the performane of a conversational dialog system, as the range of possible responses which are appropriate is extremely huge. We were aware of this problem from the beginning, why we put some thoughts into a new metric based on the Sent2Vec library. However, the results of assessing the performance of the models with this metric did also not bring the good results we wished to achieve. One interesting fact we could extract from the results was, that the Reddit model was twice as good as the OpenSubtitles model in this evaluation. We concluded that this has to do with the differences between spoken and written language, which are both present in our datasets. Overall, the results did in our opinion not reflect the performance of the models because we had already talked to them and thought that the results were not that bad. We then started to investigate into the language models to find the reason for the poor results.

Our first try to find a reason for the poor results was to look at the generated responses. We quickly noticed the lack variety in the responses. Instead our models often generated generic sentences, such as ``i don t know'' and ``i m not sure''. We started to investigate into the how our models adopted the language models from the training datasets. We did this by looking at the development of uni- and bi-gram distributions over the time span of the training. This revealed, that the learning process of the OpenSubtitles model differed from the learning process of the Reddit model. It exhibited much more variance, which we concluded is due to the much more noisey dataset in comparison to the Reddit dataset. It also revealed, that it is not recommendable to use smaller dataset and iterate over it multiple times, as we did with the Reddit model. The result was, that the snapshots of the later Reddit models showed a much poorer performance overall, for metrics and if we talked to the model directly.

We still had the assumption that the generic responses could be explained somehow. This is why we did another analysis were we analyzed the usage frequencies of the top 10 most used uni-, bi-grams and sentences in the outputs when using the test datasets. Our intention with this analysis was that we wanted to see how the language variety evolves over time. The results showed that the amount of generic responses decrease over time. We could also confirm our subjection impression that the models have a tendency to become better over time, especially the OpenSubtitles. For the Reddit model, we could conclude through this analysis that the language variety and hence the performance of the model is at its peak when using the 1.5M or 2.0M snapshots. In the end we did not find a definitive answer to the question what the reasons for the generic answers are. However, we could show, that with time, this problem reduces as the models have more time to learn.

After all the analysis, we started to compare our models to two other, namely the CleverBot and the results from the paper mentioned at the beginning of this chapter. Our models could keep up with the CleverBot and the results from the paper, as long as the utterances did not involve complicated or indistinct sentences. In general, the results from comparing our models to the result from the paper were sober, because most of the utterances we used from the paper yielded poor results. However, this was not a surprise, as we already knew that the samples from the paper involved a lot of complicated topics, including morality and philosophy. We concluded that the main factor that the responses by our models were not as expressive and thoughtful as in the paper are the training time and the model size, which is half of that used in the paper.

As the last analysis, we took a quick look into the beam-search implementation, the soft-attention mechanism and the clustering of thought vectors. This revealed, that the results coming from the beam-search decoder are often times better then when using the greedy decoder. The analysis of the soft-attention mechanism and its impact on the generation of responses by our models showed that it does not help a lot in the context of conversational models, as already noticed by others. The clustering of generated thought vectors however showed, that our models indeed had a understanding of language, as most of the similar sentences were clustered near to one another.

In summary, we are satisfied with the outcome of this thesis, even though the results were worse than we initially expected. It gave us the possibility to dive into a complete new and previously unknown topic in the area of machine learning. We learned to know a vast amount of new theories, research areas and architectures, while simultaneously allowing us to implement a system to solve a really interesting task. We learned a lot about the most recent innovations in the field of dialog systems and would liked to have much more time for this thesis, as this would have allowed us to dive even deeper and use more time for training the models and the analysis of the results.

\chapter{Future Work}

To finish our theses, we would like to give several leads how the developed system and models could be extended in a future work:

\begin{itemize}
  \item The first, obvious change we would have liked to make is to extend the time period used for training. Such models need a lot of time to be trained and as our analysis and also our impression has shown, it would be advisable to train the mentioned models for a longer period than only three weeks. We cannot say anything about what the most appropriate time period would be. This is especially true for the OpenSubtitles model as we were only able to process a little more than half of the training dataset. For the Reddit model, the size of the dataset would obviously have to be extended in order that such a long training makes sense.
  
  \item The second, maybe obvious point, is to grow the size of the model itself. This is not easily possible, as we already used the largest size possible so that the model still fit on a single GPU. However, the implementation of the models could be extended to support multi-GPU use-cases. This would require a major redesign of the implementation of the models, as the computional graph would have to be split across the multiple GPUs. This is possible in TensorFlow, via the \texttt{tf.device} API\footnote{https://www.tensorflow.org/api\_docs/python/tf/device}, which allows for running the computations across multiple different devices (i.e. GPUs, CPUs). Such a split was already done by other people, for example in one of the original sequence-to-sequence learning papers by Sutskever et al \cite{Sutskever:2014}.

  \item We had to fix the size of our computational graph to use 30 time steps for the encoder and decoder each. This is due to the fact, that we had to use an old TensorFlow API (see Chapter~\ref{software_system:development_history}) because we were running into huge problems when first developing our system. Another, also obvious change, would be to use the dynamic RNN API\footnote{https://www.tensorflow.org/api\_docs/python/tf/nn/dynamic\_rnn} from the most recent TensorFlow version to implement the models.
  
  \item Another point is to try different RNN cells instead of LSTM. For example, Gate Recurrent Units \cite{Chung:2014} or even Convolutional LSTM \cite{Xingjian:2015} cells could be used. Another idea is to try other extensions to LSTM cells, for example by using peephole connections.

  \item We located the problem, that our model often generates generic responses. This could be tackled by using dedicated loss function which trains the models with an incentive on language variety, such as the loss function found in the paper by Li et al. \cite{Li:2016}.

  \item One of the main issues we had were the metrics to asses the performance of such conversational models. We are currently not aware of any other popular metrics used in the field, besides the cross-entropy loss and the perplexity. However, it would be certainly possible and a really interesting research topic to develop new metrics, such as we tried with our Sent2Vec metric.

  \item Another interesting point would be the possibility to formulate the sequence-to-sequence learning as a beam-search optimization problem as Wiseman et al. \cite{Wiseman:2016} did.

  \item A point which we have not covered at all in this thesis is context between the utterances within a conversation. This is a hard problem, as the models would have to store the context somehow between the utterances.\todo{Find some papers!}
\end{itemize}
