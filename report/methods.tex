\chapter{Methods}
\label{methods}
In this chapter, we will elaborate on how we performed the experiments, which hyperparameters where used, and how we evaluated the results of the trained models.

\section{Architecture of the Sequence-To-Sequence Model}
In the following paragraphs, we are going to describe the architecture of the model used to conduct our experiments.\todo{rework chapter introduction}

\paragraph{Sequence-To-Sequence} In general, we are using the architecture of seq2seq models described in Chapter~\ref{fundamentals:seq2seq} with LSTM cells. We restrict ourselves to use only one LSTM cell for the encoder, and another cell for the decoder. This is because we wanted our cells to be as large as possible to come as close to the size of the cells used in the paper by Vinyals and Le\cite{Vinyals:2015}. However, they used two really large cells with each having a hidden state size of $4096$ hidden units and a vocabulary consisting of $100,000$ words. In the paper, they had to train their models on a large number of CPUs, because such a huge network does not fit in the memory of any currently available GPU. Since we want to train our models on a single GPU (see Chapter~\ref{sofware_system:development_history}), we had to shrink the size of our model to the largest size possible so that it still fits within the ~12GB of memory that our GPUs have (see Chapter~\ref{software_system:hardware}). The exact size of the model used in this thesis is described in ``Hyperparameters'' below.

\paragraph{Down-Projection of Hidden State} As we have seen above, very large RNN cells are problematic. So we implemented what is known as a \emph{down-projection} at the end of the decoder cell. This is done similar to the down-projection used in \cite{Vinyals:2015}, with the main difference lying in the motivation why we implemented it. In the paper, they state that they used the down-projection technique to speed up the training due to the large weights-matrix in the softmax layer at the end. In our case, the down-projection was not just for speeding up the training, but mainly to allow us to use larger cells than we would otherwise be able to. The implementation of this feature allowed us to grow our model by a factor of two, from a hidden state size of $1024$ to $2048$\todo{units?}, without having to sacrifice the size of the vocabulary (see Chapter~\ref{methods:hyperparameters}).

\paragraph{Sampled Softmax} To speed up the training of the large softmax layer at the end (consisting of $50'000$ entries), we use a \emph{sampled softmax} as described in \cite{Sebastien:2014} which is only applied while training the models. The idea is that, instead of using the full vocabulary for the softmax at each time step, we only use a small random sample (which, must, however, obviously contain the target word). This speeds up training dramatically due to the fact, that not the whole softmax layer has to be used and hence not all derivates have to be computed. On inference time, when we generate predictions, we have to use the full softmax layer again to generate the predictions.

\paragraph{Static Unrolling of RNN} Because we are working with a deprecated \texttt{TensorFlow} API (see chapter \ref{sofware_system:development_history}), we have to use static unrolling for our model. Static unrolling works by defining a fixed size of time steps for the encoder and decoder, and then unrolling the encoder and decoder cell for this number of time steps \emph{before} we are actually computing anything using it. This actually ``removes'' the recurrence from our model and transforms it into kind-of ``feed forward'' model with the major difference being that the weights are shared between the layers (as each layer represents the same cell). With the latest \texttt{TensorFlow} API\footnote{https://www.tensorflow.org/api\_docs/python/tf/nn/dynamic\_rnn}, it is possible to create the graph for the encoder dynamically at runtime based on the length of the input. The decoder, in the dynamic case, then runs for as many time steps as necessary to either reach an \texttt{EOS} token or the maximum number of time steps allowed to decode.

\begin{figure}
	\label{methods:static_unrolling:unrolled_rnn}
	\centering
	\includegraphics[width=10cm]{img/rnn_unrolled}
	\caption{Image for illustrating unrolling an RNN over a fixed size of time steps.\protect\footnotemark}
\end{figure}
\footnotetext{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}

This implementation forced us to define a maximum number of time steps to use for the encoder and decoder beforehand, which we have set to $30$ for both the encoder and decoder. This length is also mainly restricted due to the fact that the longer the time span becomes, the larger the memory consumption on the GPU becomes. We tried several different sizes ranging from $10$ to $100$ time steps and evaluated that $30$ would be the largest possible size without being required to shrink any other parts of the model.

\paragraph{Reversing the Input Sequence} As Sutskever et al.~\cite{Sutskever:2014} have noticed in their paper, it was beneficial to reverse the input sequence before feeding it to the model. They assume, that this helps because the minimal time lag~\cite{Hochreiter:1997} is greatly reduced due to the fact that the first few words in the source language are much closer to the first few words in the target language. This explanation makes sense in the context of machine translations, but not necessarily in the context of a conversational dialog-system. Nevertheless, we want to try to use it and see if it makes any difference our models. We decided that the Reddit model should be the one that is trained with reversed input sequences.
\todo{wenn wir in den Resultaten OpenSubtitle reversed angeben, dann müssen wir das hier präzisieren}

\section{OpenSubtitles and Reddit Models}
\label{methods:both_models}
As already described in Chapter~\ref{chapter:data}, we are going to train two distinct models that each use a separate training, validation and test dataset. The first is trained on the OpenSubtitles dataset and the second on the Reddit dataset (see Chapter~\ref{chapter:data}). Both of them are trained in the same way for the same time period, except for the difference regarding how the input sequences are fed to the model.

\section{Hyperparameters}
\label{methods:hyperparameters}

\paragraph{Model} The hyperparameters used for training the models can be seen in Table~\ref{methods:hyperparameters:table}. Both of our models use the same set of hyperparameters, the only difference being that the Reddit model is fed with reversed input sequences whereas the OpenSubtitles model is not.

\begin{table}[H]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{ll}
			\toprule
			Name & Value\\ \midrule
			Number of encoder cells & $1$\\
			Number of decoder cells & $1$\\
			Max. number of time steps in encoder & $30$\\
			Max. number of time steps in decoder & $30$\\
			Hidden state size & $2048$\\
			Projected hidden state size & $1024$\\
			Number of sampled words for softmax & $512$\\
			Size of the softmax layer & $50'000$\\
			Batch size & $64$\\
			Max. Gradient Norm for Clipping & $10.0$\\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
	\caption{Hyperparameters used for our seq2seq models.}
	\label{methods:hyperparameters:table}
\end{table}

\paragraph{Optimizer} As the optimizer, we use \emph{AdaGrad}~\cite{Duchi:2011} as in~\cite{Vinyals:2015} with the learning rate set to $0.01$. We also use gradient clipping \cite{Pascanu:2013} and set the maximum allowed gradient norm value to be $10$.

\section{Training}
\label{methods:training}
We train our models on the hardware described in Chapter~\ref{software_system:hardware} with the software packages from Chapter~\ref{software_system:softwar_packages}. As this models take a long time to achieve progress (in~\cite{Vinyals:2015} the authors trained their model for several weeks), we also have to train ours for a prolonged time period. To show the development of the model throughout this time, we take several snapshots, approximately every third day or about after every $500,000$ batches. Each of this snapshot has a distinct name indicating at which point in the training it was created. The names are going to follow the naming convention xM, where x stands for the decimal number signifying the number of batches processed in millions (e.g. 0.5M stands for $500,000$ and 1.5M stands for $1,500,000$ batches prcoessed). The taken snapshots are going to be used in the analysis we are going to do in the next chapter. They enable us to not only do an analysis of the final results, but rather show the evolution of the models throughout the training process. The two models were trained over a time period of 20 days, each trained on roughly 3 million batches of 64 samples each.

As the two training datasets are not equal in size, using the same time period for the training of both models induces that they see different shares of their respective training datasets. In the case of the OpenSubtitles model, the case is that it can only process 192 million samples, which equals to about 59\% percent of the whole OpenSubtitles training dataset. In the case of the Reddit model, the two week training allows doing two and a half epochs over the entire dataset. This allows us to analyse the difference between using the approaches of using a single big dataset, which we are not able to completely process, against a smaller corpus that we can iterate more than twice in the same time.

\section{Evaluation}
\label{methods:evaluation}
We use two different kinds of evaluation: Metric-based and human-based. The first one is a quantitative analysis of the results and consists of analyzing the training process and the final results under the metrics defined in Chapter~\ref{fundamentals:metrics}. We used the test datasets generated as described in Chapter~\ref{data:split_corpus}. Due to the sheer size of the datasets, we only use the top $250,000$ samples from the test datasets for our evaluation. We are also going to do an n-gram analysis, as already mentioned in Chapter~\ref{chapter:data:ngram}, where we analyze the language model of the resulting models with regard to the language used in the corpora. This also includes an analysis of the syntax and semantics of the outputs generated by the trained models. The human evaluation is done by judging how ``meaningful'' the texts generated by the resulting model are and we compare them with results found in other papers (e.g. \cite{Vinyals:2015}) and chatbots (such as CleverBot\footnote{http://www.cleverbot.com/}).

We also considered using the BLEU and ROGUE metrics for evaluating our models. After some research, we have found that this metrics are primarily used in the context of machine translations and work by comparing the resulting texts to several reference translations. In our opinion, this does not make a lot of sense in the context of conversational models since the answers to a specific input can have a wide variety of possible forms that are all meaningful and fitting but do not necessarily need to be alike the reference response. This thought is supported by results that Nguyen et al. \cite{Nguyen:2016} have found in their paper, where they state that they used the BLEU and ROGUE metrics for evaluating their chatbot and determined that these metrics are not fit to do such an evaluation because of the aforementioned reasons. Instead of the mentioned similarity metrics, we are going to use Sent2Vec~\cite{Pgj:2017}, which allows us to do similarity measures between the expected and generated outputs of the model using a highly dimensional vector space where the sequences can be embedded. This can be done by using the pretrained models found in the GitHub repository\footnote{https://github.com/epfml/sent2vec}, as it is possible to embed new sequences without the requirement of retraining the models. The exact algorithm for the evaluation using Sent2Vec can be found in Chapter~\ref{fundamentals:metrics}.
