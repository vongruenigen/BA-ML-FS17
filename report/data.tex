\chapter{Data}\label{chapter:data}
Dieses Kapitel beinhaltet Informationen über die Datensets. Auch erklären wir hier unsere Vorgehensweise, wie wir von den Rohdaten zu unseren "Trainingsdaten"(todo Begriffe abstimmen) kommen. For the following experiments we used two datasets: The OpenSubtitle dataset includes move subtitles, so this dataset represents spoken language. On the other hand we have the Reddit Submission Corpus dataset which contains written language. We choose those movie related datasets because we focus all subject about movies and films.

\section{Original datasets}
In diesem Abschnitt befinden sich Informationen zu den rohen Datensets. In the table \ref{table_data-origin} you can see some general information about the datasets.
\paragraph{OpenSubtitle} For each movie there exists a file which contains the movies subtitles in chronologically order. You can see en example in the picture \ref{?}.
\begin{table}[H]
	\centering
	\small
	\begin{adjustbox}{max width=\textwidth}
	\begin{tabular}{llllll}
		\toprule
		&  \specialcell{\emph{short name}}
		&  \specialcell{\emph{size} \\\textit{[GB]}}
		&  \specialcell{\emph{lines} \\\textit{[million]}}
		&  \specialcell{\emph{data format}}
		&  \specialcell{\emph{source}} \\
		\midrule
		\emph{OpenSubtitle 2016}						&Opus	& 93	& 338	& XML	& \cite{lison2016opensubtitles2016}	\\
		\emph{Reddit Submission Corpus 2006-2015} 		&Reddit	&885	& 1'650	& JSON	&  Reddit Comments Corpus \protect\footnotemark \\
		\bottomrule
	
	\end{tabular}
	\end{adjustbox}
	\caption{Origing and some general information about the Datasets.}
	\label{tbl:data:rawData}
\end{table}
\footnotetext{$https://archive.org/details/2015_reddit_comments_corpus.$}
\section{Preprocessing}
In diesem Abschnitt wird erklärt, wie wir die Daten für das Training entsprechend extrahieren, filtern und anschliessend die Dialoge zusammengebaut werden.
\subsection{Extraction}
Was die erlaubten Zeichen betrifft, so gelten für beide Datensaätze die gleichen folgenden Regelen:
\begin{itemize}
	\item Gültige Zeichen sind a-z, A-Z, 0-9 und die Satzzeichen .,!?
	\item Wörter welche ungültige Zeichen beinhalten werden entfernt
	\item Zwischen Wörter und Satzzeichen wird immer ein Leerzeichen eingefügt
\end{itemize}
\paragraph{OpenSubtitle} Die Daten liegen sortiert nach Film Genre und Jahr vor. In diesen Unterordner befindet sich pro Film eine Datei welche im XML Format den Untertitel beinhaltet.
In einem ersten Schritt werden die gezippten Files in den Unterordner gesucht. Anschliessend wird jedes File entzippt und mit Hilfe der XML Library eingelesen. In der Abbildung \ref{?} ist ersichtlich, dass die einzelne Kinder des XML Objekts die Wörter enthalten. Die Wörter werden ausgelesen und entsprechend zu einem Satz kombiniert. Im der Abbildung \ref{?} sehen Sie den oben gezeigten Beispielsatz nach dem Preprocessing. Ein Dialog wiederum wir aus zwei nacheinander folgenden Sätzen erstellt.
\paragraph{Reddit Submission Corpus 2006-2015} Die Reddit Daten sind nach Jahr und Monat sortiert. Pro Monat gibt es eine Datei mit chronologisch aufsteigend sortierten JSON Objekte.
Als erstes werden, abhängig von den Programmierter, die Dateien pro gewünschtes Jahr zusammengesucht. Anschliessend werden diese Dateien aufsteigend mit Hilfe der JSON Library ausgelesen und nach entsprechenden Subreddits gefiltert. Ein Beispieleintrag finden Sie in der Abbildung \ref{?}. Da wir ein Dialog System mit Fokus auf Filme erstellen möchten, filterten wir die Daten entsprechend nach den Subreddit Tags, Movies, Films und Television. Nach dem Filtern der Daten, muss die Ursprüngliche Struktur der Kommentare wiederhergestellt werden. Für diesen Vorgang sind die Tags, $Parent_id$, $Link_id$ und Name relevant. Auf der obersten Kommentarebene ist die $Parent_id$ gleich der $Link_id$. Wobei beim ersten Vorkommnis pro ID ein neuer Beitrag beginnt. Für die Kinder eines Kommentars verhält es sich so, dass deren $Parent_id$ gleich dem Name Tag des Elternkommentar ist. Mit Hilfe der Shelve Library werden so die Urpsrünglichen Kommentarstrukturen erstellt. Dialoge werden generiert, indem der Elternknoten jeweils mit allen direkten Kindern kombiniert wird. In der Abbildung \ref{?} sind 2 Beispielkonversationen ersichtlich. Zwischen den Konversationen wird ein Trennzeichen ('<<<<<END-CONV>>>>>') eingefügt, welches in den Experimenten dem Modell entsprechend das Ende eines Dialogs anzeigt.
Todo: Erklärendes Bild einfügen wie zusammengesetz wird, und referenz paper die das auch so zusammengebaut haben

Pro Datensatz erhalten wir nach dem Preprocessing eine Datei, welche pro Zeile eine Aussage enthält.
todo: Clique Bildung in text einbauen? entscheid für subreddit tags Visualisierung der CLique gemäss BigQuery
todo: visualisierung bildung der dialoge reddit?
\section{Generate Vocabulary and a train, valid and text Dataset}
Da wir die Wörter nicht normalisieren können, müssen wir die zur Verfügung stehenden Wörter begrenzen. Ansonsten würde bei der Softmax Berechnung zu wenig Speicher zur Verfügung stehen(Genauer erklären, referenz auf Kapitel wo erklärt wird). Dementsprechend wird das Vokabular spezifisch für jeden Datensatz generiert.
Dabei werden zuerst alle vorkommenden Wörter gezählt und nach der Häufigkeit absteigend ausgegeben. Wir extrahierten daraus 3 Vokabulare, mit den jeweils 25k, 50k und 100k häufigsten Wörter. Die anschliessende Analyse zeigt die Abdeckung des Datensatzes pro Vokabular. Die Ergebnisse sind in der Tabelle \ref{?} ersichtlich.

Wir sehen also, dass trotz der grossen Anzahl Wörter (Angabe Wörter insgesamt pro Dataset), können bereits mit 50k Wörter \% der Wörter abgedeckt werden.
Ebenfalls Analyse Wieviele Wörter pro Satz Prozentual
todo: Einfügen Tabelle Abdeckung Vocab => Korpus
todo:zahlen in Text einfliessen lassen einfügen und pickl generierung?
\begin{figure}[h]
	\label{fig:data:analyse:timediff:opus}
	\centering
	\includegraphics[width=6cm]{img/dummy.png}
	\caption{Absolute Verteilung der Sätze pro Zeitabstand. \protect\footnotemark}
\end{figure}


Wir untersuchten die Daten zusätzliche nach Häufigkeiten von Bi- und Trigrammen. Diese Vorgang war sehr rechenintensiv und dauerte entsprechend lange. Die Resultate sind pro Datensatz in den Abbildungen \ref{?} und \ref{?} ersichtlich. Das Ziel dieser Analyse ist es, die später generierten Sätze ebenfalls auf Bi- und Trigramme zu untersuchen um eine mögliche Korrelation auf der Ebene Phrase zu finden.
Todo: Wolkenbildung...

Schlussendlich teilten wir die Datensätze auf wobei 97\% als Trainingsdaten, 2\% als Validierungsdaten und 1\% als Testdaten verwendet werden. Die genauen Zahlen befinden sich in der Tabelle \ref{tbl:data:split:corpus}.
\begin{table}[H]
	\begin{adjustbox}{max width=\textwidth}
	\centering
	\small
	\begin{tabular}{llllllll}
		\toprule
		&  \specialcell{\emph{vocab size}	\\\textit{[thousand]}}
		&  \specialcell{\emph{total amount}\\\emph{words} \\\textit{[\%]}}
		&  \specialcell{\emph{single coverage} \\\textit{[\%]}}
		&  \specialcell{\emph{0 missing words}\\\emph{per utterance} \\\textit{[\%]}}
		&  \specialcell{\emph{1 missing words}\\\emph{per utterance} \\\textit{[\%]}}
		&  \specialcell{\emph{2 missing words}\\\emph{per utterance} \\\textit{[\%]}}
		&  \specialcell{\emph{total coverage} \\\textit{[\%]}}\\
		\midrule
		\emph{Opus}			&25		&-	&-	&-	&-	&-	&-\\
							&50		&-	&-	&-	&-	&-	&-\\
							&100	&-	&-	&-	&-	&-	&-\\
		\emph{Reddit}		&25		&-	&-	&-	&-	&-	&-\\
							&50		&-	&-	&-	&-	&-	&-\\
							&100	&-	&-	&-	&-	&-	&-\\
		\bottomrule
	\end{tabular}
	\end{adjustbox}
	\caption{single coverage abdeckung einzelner wörter. sollte prfübar sein, total words amount mit covab grösse kombiniert.}
	\label{tbl:data:split:corpus}
\end{table}

Beim Durchsichten der Daten, empfanden wir die Qualität des Opus Datensatz als eher schlecht. Wir vermuteten die Ursache in der gesprochenen Sprache, da das Bild als wichtiger Informationsträger fehlt. Eine zusätzliche mögliche Begründung wäre es, dass die Bildszene endet und mit einigem zeitlichen Abstand erst wieder gesprochen wird. Somit würden Sätze kombiniert werden, welche nicht den gleichen Kontext besitzen. Deshalb haben wir die zeitlichen Abstände zwischen den Sätzen analysiert. Die Ergebnisse sind in der Grafik \ref{fig:data:analyse:timediff:opus} ersichtlich. Aufgrund der Ergebnisse, dass über 80\% der Aussagen in einem Zeitabstand kleiner gleich 5 Sekunden aufeinander folgen, verwerfen wir vorerst diese These als Ursache. In der Abbildung \ref{fig:data:analyse:timediff:opus} sind die Werte grösser 30\% auffällig hoch. Dies liegt einerseits an den Übergängen zwischen 2 Datensets und weil alle restlichen Werte in dieser Klasse landen. Dies sehen wir aber nicht als Problem an, weil über 80\% der Daten diese Problematik nicht betrifft.
Wir unternehmen vorerst keine weiteren 

\begin{figure}[h]
	\label{fig:data:analyse:timediff:opus}
	\centering
	\includegraphics[width=15cm]{img/opus_time_analyze.PNG}
	\caption{Relative Verteilung der berechneten Zeiten (Diskret) in ganzen Sekunden zwischen zwei Äusserungen. Wobei die grössten Anteile die folgende 5 Zeitabstände haben: 13.0\% 1 Sekunde, 26.4\% 2 Sekunden, 22.8\% 3 Sekunden, 13.4\% 4 Sekunden und 7\% 5 Sekunden. Die resultierende Abdeckung dieser 5 häufigsten Zeitabstände beträgt 82.6\%. \protect\footnotemark}
\end{figure}


Wir untersuchten die Daten zusätzliche nach Häufigkeiten von Bi- und Trigrammen. Diese Vorgang war sehr rechenintensiv und dauerte entsprechend lange. Die Resultate sind pro Datensatz in den Abbildungen \ref{?} und \ref{?} ersichtlich. Das Ziel dieser Analyse ist es, die später generierten Sätze ebenfalls auf Bi- und Trigramme zu untersuchen um eine mögliche Korrelation auf der Ebene Phrase zu finden.
Todo: Wolkenbildung...

Schlussendlich teilten wir die Datensätze auf wobei 97\% als Trainingsdaten, 2\% als Validierungsdaten und 1\% als Testdaten verwendet werden. Die genauen Zahlen befinden sich in der Tabelle \ref{tbl:data:split:corpus}.
\begin{table}[H]
	\begin{adjustbox}{max width=\textwidth}
	\centering
	\small
	\begin{tabular}{llllll}
		\toprule
		&  \specialcell{\emph{set}}
		&  \specialcell{\emph{Percent from total} \\\textit{[\%]}}
		&  \specialcell{\emph{size} \\\textit{[GB]}}
		&  \specialcell{\emph{lines} \\\textit{[million]}}
		&  \specialcell{\emph{Dialogue Examples} \\\textit{[million]}}\\
		\midrule
		\emph{Opus}			&train	&-	&-	&-	&-	\\
							&valid	&-	&-	&-	&-	\\
							&test	&-	&-	&-	&-	\\
		\emph{Reddit}		&train	&-	&-	&-	&-	\\
							&valid	&-	&-	&-	&-	\\
							&test	&-	&-	&-	&-	\\
		\bottomrule
	\end{tabular}
	\end{adjustbox}
	\caption{In this table you can see the resulting 3 sets per Dataset....}
	\label{tbl:data:split:corpus}
\end{table}
todo: split erklären, 


Preprocessing
Regex, Filter bei Reddit 2 Varianten, nach Subrreddit, 30 Wordcount


Generiereung Vokabular und Abdeckung pro Vokabular und corpus

Bi-Gram Tri-Gram Analyse entweder mit WOlke, oder heat mat, tree map
Vielleicht gibt es noch eine andere Variante um dies darzustellen (besser), wenn möglich mit paper referenz
$http://infosthetics.com/archives/2006/03/subject_tag_news_heat_map.html$
$https://www.quora.com/What-are-alternatives-to-tag-clouds-for-information-visualization$

Begriffe: Utterance (eine Aussage)
			Sample (in- utterance, und out utterance)
			
Beispiel Opus: Vor und nach preprocessing:
Tae Gong Sil was the 'big sun' , and you're 'little sun'. => tae gong sil was the big sun, and you re little sun .
♫ I'm picturing you with my tears ... ♫ You are engraved on my heart ... => i m picturing you with my tears , you are engraved on my heart
'JACKIE - - YOU KNOW , I HAVE BEEN AT EVERY FIGHT , MEGAN - - ==> jackie you know , i have been at every fight , megan'