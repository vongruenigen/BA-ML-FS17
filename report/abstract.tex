\chapter*{Abstract}
In the following thesis we are going to investigate into building a end-to-end conversational dialog system based on a sequence-to-sequence learning and recurrent neural networks. We use an already built system as the reference architecture. The main topic of the research is to investigate if it is possible to use model, which can be trained on a single GPU, and still exhibit similar performance as much larger models.

The first parts are dedicated to the development of the software system and the datasets and methods used to conduct the experiments.

The analysis of the resulting models is done under several aspects. First, we are going to look into the learning process and try to distinguish differences between using two different datasets for the training. It is shown that the structure and linguistic nature of different datasets has a clear impact on the learning process of such models. Evaluations to asses the performance follow afterwards, which includes tests with a newly proposed metric based on the Sent2Vec library.

The previous analysis reveals several problems our models exhibit. One of the biggest is that such models tend to generate generic responses which leads to deteriorating results. This does not match with our subjective impression that the model indeed become better over time. To find an explanation for this behavior we investigating into the way the language used by this models evolves over time. This analysis shows that language variety increases with training time and the problem with generic responses decrease. We also show that it is not advisable to use small datasets for multiple epochs of the training without any other regularization methods.

A comparison with two other systems, namely the CleverBot and the results from the paper, then follows. This comparison shows that our models can keep up with others as long as the complexity of the dialogs do not get too complicated. The assumed reasons for the inferior results are the smaller size of our models as well as the shorter training time in comparison with other systems.

For the last part, we do an analysis of several technical specifics. This includes short analyzes of using a beam-search decoder, the aid the soft-attention mechanism provides to the models and the clustering of thought vectors. 

We see this thesis and the resulting models as a success, even though the results are not as good as we expected. We were able to provide multiple interesting results, such as the response ``i m a bot'' to the question ``what are you ?''.
