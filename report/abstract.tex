\chapter*{Abstract}
In the following thesis we are investigating into construction of an end-to-end conversational dialog system based on recurrent neural networks and sequence-to-sequence learning. We use an already existing system as the reference architecture. 

The main topic of the research is to investigate if a downsized model still exhibits a comparable performance if trained on a single GPU, compared to the larger system.

The first chapters are dedicated to the development of the software system and the datasets and methods used to conduct the experiments.

The analysis of the resulting models is done under several aspects. First, we are going to look into the learning process. It is shown that the structure and linguistic nature of different datasets has a clear impact on the learning process of such models. Evaluations to asses the performance follow afterwards, which includes tests with a newly proposed metric based on the Sent2Vec library for measuring semantic similarity.

The previous analysis revealed several problems our models exhibited. One of the main issues is that such models tend to generate generic responses which leads to deteriorating results. This does not match with our subjective impression that the model indeed become better over time. To find an explanation for this behaviour we investigate how the language, used by the the models, evolves over time. This analysis shows that language variety increases with training time and the occurrence of generic responses decreases.

A comparison with two other systems, namely the CleverBot and the results from the ``Neural Conversational Model'' paper, follow subsequently. This comparison shows that our models can deliver comparable results as other models, as long as the dialogs have limited complexity. The assumed reasons for the inferior results from our models are the smaller model size as well as the shorter training time, compared to other models.

In the last part, we conduct an analysis of several technical specifics. This includes short analyzes of using a beam-search decoder, the assistance the soft-attention mechanism provides to the models and the clustering of thought vectors. 

Although the results do not match our expectations, we are satisfied with the work. The system is able to, at least partially, generate meaningful dialogs, such as the response ``i m a bot'' to the question ``what are you ?''.
