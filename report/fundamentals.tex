\chapter{Fundamentals}
\label{chapter:fundamental}
In this first chapter, we lay out the fundamentals used in the rest of this thesis. At the beginning, we will introduce basic definitions such as \emph{utterance}, \emph{sample} and \emph{conversation}. We will then follow up with an introduction into machine learning using a special variant of \emph{Neural Networks} (NN), called \emph{Recurrent Neural Network} (RNN). We show the basic principles behind it and explain problems this RNNs have in practice, and how the can be solved by utilizing other forms of RNNs. We then show how RNNs can be used to build models, called \emph{Sequence-To-Sequence} (seq2seq) models \cite{Sutskever:2014}, which are capable of learning and using a language in a conversational context.

The following introduction only covers a small part of the spectrum of possibilities with regard to the tasks these models can perform. Basically, we are restricting our explanations to the supervised learning use-case and ignore the unsupervised ones, even though they have a wide area of applications (e.g. dimensionality reduction, regression) as shown by !REFERENZ FÃœR UNSUPERVISED LEARNING?!. A basic introduction into the principles of neural networks can be found in appendix \ref{fundamentals:neural_networks}.

\section{Definitions}
\paragraph{Utterance} \blindtext
\paragraph{Sample} \blindtext
\paragraph{Conversation} \blindtext

\section{Recurrent Neural Networks}
\emph{Recurrent neural networks} (RNN) are a special variation of the NNs described in appendix \ref{fundamentals:neural_networks}. The main difference between them is, that RNNs have a recurrence built into it, which allows them to adapt to problems which also have a temporal dimension and are dependent on data from differente timesteps to solve. We're also going into the problems such RNNs have due to their recurrence and how this problem can be solved by exploiting another form of RNN, called \emph{Long Short-Term Memory Networks} (LSMT).

\paragraph{RNNs in principle}
One of the main restrictions of vanilla NNs is the following: Assume a task, where the NN is conditioned to output a prediction on tomorrows weather given yesterdays. Now, if one would use a vanilla NN as described in the appendix, the main restriction would be that the NN has to make its prediction solely based on the weather information of the day before. Such a model does not take into account, that weather is not only dependent on the weather of the previous day, but also on the days before. This could be solved by feeding, say, the weather of the last week to the network instead of just the weather of the previous day. But if new scientific evident now shows, that weather is not only dependent on last week, but also on the last month, probably the last year, we quickly get into problems due to the sheer size of a NN performing such tasks, because the input size grows rapidly. Also, such a NN would still be static in the way, that one cannot simply change the time-window used to feed to the network. If one settles with one month, it will always be able predict the weather based on the last month, but not any different time-window, otherwise the NN has to be retrained using another time-window in order to work again as before. RNNs (see figure \ref{fundamentals:rnn:rolled_vanilla}) try to solve this problem by introducing a recurrence into the network, which allows it to exploit informations not only from the current input, but also from inputs of the past. In a more formal way, one could say that this recurrence allows RNNs to ''exhibit dynamic behaviour across the temporal dimension of the input data''.

\begin{figure}[h]
	\label{fundamentals:rnn:internal_structure}
	\centering
	\includegraphics[width=10cm]{img/rnn_internal}
	\caption{Internal structure of a vanilla RNN.\protect\footnotemark}
\end{figure}
\footnotetext{http://r2rt.com/static/images/NH\_VanillaRNNcell.png}

Before explaining how this recurrence can be used to solve the weather prediction problem, let's first show the equations used for the forward propagation in a RNN with a single cell. A single layer in an RNN is called a \emph{cell}. 

The internal structure is similar to the one of NNs, as explained in appendix \ref{fundamentals:neural_network}, except for the recurrence which we're going to elaborate on below.

\begin{equation}
\begin{split}
h_t & = \varphi_h(\mathbf{w}_h \cdot \mathbf{x}_t + \mathbf{u}_h \cdot \mathbf{h}_{t-1} + \mathbf{b}_h) \\
    & = \varphi_h\bigg(\sum_{i=0}^{n} w_i x_{ti} + \sum_{i=0}^{n} u_i h_{t-1i}\bigg)
\end{split}
\label{fundamentals:rnn:forward_equation:hidden}
\end{equation}

\begin{equation}
\begin{split}
o_t & = \varphi_y(\mathbf{w}_o \cdot \mathbf{h}_t + \mathbf{b}_o) \\
    & = \varphi_y\bigg(\sum_{i=0}^{n} w_i x_{ti} + \sum_{i=0}^{n} u_i h_{t-1i}\bigg)
\end{split}
\label{fundamentals:rnn:forward_equation:output}
\end{equation}

In the equations above, one can see different variables with different indices which we'll explain briefly:

\begin{itemize}[noitemsep]
	\item $x_t$ stands for the input at time-step $t$.
	\item $o_t$ stands for the output of the network at time-step $t$.
	\item $h_t$ stands for the hidden state at time-step $t$.
	\item $\mathbf{w}_h$, $\mathbf{w}_o$ and $\mathbf{u}$ are the weight matrices learnt while training the model.
	\item $\mathbf{b}_h$ and $\mathbf{b}_o$ stand for the bias vectors used when computing the new hidden state or output respectively.
	\item $\varphi_y$ and $\varphi_h$ are the activation functions used to compute $o_t$ and $h_t$ respectively.
\end{itemize}

As seen above, at every time step $t$, the new hidden state $h_t$ and the output $o_t$ are computed. The value of $o_t$ can be seen as the prediction of the RNN at time step $t$ and the value of $h_t$ is the value that is passed to the same cell in the next time step $t+1$ for computing the next output $o_{t+1}$ (and hidden state $h_{t+1}$). As depicted in figure \ref{fundamentals:rnn:internal_structure}, the hidden state $h_t$ is returned to the cell for computing the output $o_{t+1}$ and $h_{t+1}$. This is what an RNN allows to exhibit behaviour depending on data seen in past time steps: It can ''remember'' what it has already seen and the information necessary for the prediction in the hidden state, which is passed along the temporal dimension when a RNN is run through multiple steps in time.

To come back to our weather prediction problem, this allows the cell to remember what weather it has seen in the past (e.g. rainy, sunny, high pressure, low pressure) and adjust its future predictions accordingly. The recurrence also solves the problem of time windows of differente sizes: Since the recurrence allows the model to store the required informations in the hidden state at each time step and combine it with what it has already seen to make future predictions, it is not constrained to a fixed size of inputs required to make a prediction but instead uses all information it has already seen, no matter if the seen information is from the past 5 days or past 5 years, it can compress all this into the hidden state $h_t$.

While this is the solution for our problem of different window sizes, it is also a main bottleneck of the model: If one imagines, that the time window is past information is 5 years and we process these informations on a daily basis, we would need to remember the informations from around 1800 days. If we would now have 10 distinct features per day, this leads to 18'000 features to ''remember'' in total. Remembering means to ''compress'' the information into the hidden state, which is usually much smaller than 18'000, reasonable values vary from 128 up to 4096, depending on the structure and complexity of the problem. Another obstacle is, that the RNN cell has no way of ''forgetting'' informations stored in the hidden state easily, as the equation \ref{fundamentals:rnn:forward_equation:hidden} only does an addition to the hidden state $h_{t-1}$. Of course, in theory it is still possible that the RNN learns to forget insignificant informations, but this is a really hard problem in practice, why we are going to introduce yet another form of RNN cells, called \emph{Long Short-Term Memory} cells, which not only solves this problem, but also the problem of vanishing and exploding gradients explained in the next paragraph.

\paragraph{Vanishing / Exploding Gradient Problem} The recurrent nature of RNNs can cause problems in practice, one of the most problematic is the vanishing/exploding gradients problem: While training such networks, we condition the weight matrices $\mathbf{w}_h$, $\mathbf{w}_o$ and $\mathbf{u}$ via backpropagation by using gradient-descent in such a way, that the loss function is minimized w.r.t. to these parameters for the given training samples. This means, that we have to compute the derivates of the loss function by using the chain-rule $\frac{dz}{dx} = \frac{dz}{dy}\frac{dy}{dx}$. The entirety of all derivations w.r.t. to the parameters is called the gradient. Since the gradient also flows through the activation functions $\varphi_h$ of the recurrence, this computations also include the derivations of this function. Most of the commonly used activation functions today have codomains in $\mathbb{R}$ and take on values in the range of $[-1, +1]$ (e.g. $\operatorname{tanh}$), this means that multiplying a lot of this derivate values possibly leads to an exploding or vanishing values. This has a severe effect on training the RNN: If the value of the derivates is exploding, this means that the weights are adjusted in a way which we would call ''hyperbolism'' and this leads to a network where the predictions get worse and worse. On the other hand, if the values are vanishing, the network stops learning from a certain point in time on, since the weights are not really adjusted anymore, but only updated with tiny values which don't have any impact.

The problem of vanishing gradients was first analyzed by Sepp Hochreiter in his diploma thesis !!REFERENZ!!. A more formal explanation of the problem with links to dynamic systems and formal proofs, under which conditions these problems occur, can be found in \cite{Pascanu:2013}. The details on how backpropagation with gradient-descent is adapted to RNNs and time-dependent problems, called \emph{Truncated Backpropagation Through Time} or \emph{TBTT} in short, can be found in \cite{Werbos:1990}.

\paragraph{Long Short-Term Memory Networks} To solve the problem with vanishing/exploding gradients mentioned before, J\"urgen Schmidhuber and Sepp Hochreiter introduced an advanced version of a RNN cell, called \emph{Long Short-Term Memory}, or LSTM in short \cite{Hochreiter:1997}. The structure of this kind of cells is much more sophisticated then vanilla RNN cells, but it also solves the two most pressing problems: Vanishing/Exploding gradients and the inability to forget stored information.

It does this by introducing the so-called \emph{gates} into the cell, which are nothing else than small NNs which are responsible for deciding which informations from the previous time step we are going to keep in the old hidden state $h_{t-1}$, add to the new hidden state $h_t$ and use for the computation of the output $o_t$. There are three different gates in an LSTM cell (as depicted in figure \ref{fundamentals:lstm:internal_structure}):

\begin{center}
	\begin{minipage}{10cm}
		\begin{itemize}
			\item[\emph{Input gate}] Is responsible to decide which part of the input is interesting and should be used to update the hidden state $h_{t-1}$ to become the new one $h_t$.
			\item[\emph{Forget gate}] Is responsible to decide which part of the hidden state $h_{t-1}$ the cell is going to forget.
			\item[\emph{Output gate}] Decides which part of the hidden state $h_t$ is used to compute the output $o_t$.
		\end{itemize}
	\end{minipage}
\end{center}

\begin{figure}[h]
	\label{fundamentals:lstm:internal_structure}
	\centering
	\includegraphics[width=10cm]{img/lstm_internal}
	\caption{Internal structure of a LSTM cell.\protect\footnotemark}
\end{figure}
\footnotetext{http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/}

In theory, these gates do nothing more good than a plain RNN cell. However in practice this structure solves all of the aforementioned problems to at least some degree.

First, let us look into the vanishing/exploding gradient problem: The problem occured due to the fact, that the derivation of the activation function for the recurrence is used when calculating the updates of the weight parameters by performing backpropagation with gradient-descent. This can lead to vanishing (if the derivate is below 1) or exploding (if the derivate is above 1) gradients. The LSTM cell solves this problem by not applying an activation function on the recurrence, but rather using the identity function $f(x) = x$ whose derivation is always $1$. This practically solves this problem because the gradients can neither explode nor vanish.

The second problem is that an RNN cell has no easy way of forgetting information stored in the hidden state. This is obviously solved by the introduced gates which allow the cell to decide at each time step $t$, which information to keep, forget and store based on the last hidden state $h_{t-1}$ and the current input $x_t$. With this structures in place, the LSTM cell is able to track long-term dependencies without much problems.

\section{Sequence-To-Sequence Learning}
\paragraph{Model}
\begin{itemize}
	\item Explain basic idea behind model.
	\item Show different tasks which can be solved by using such models..
\end{itemize}

\paragraph{Decoding Approaches}
\begin{itemize}
	\item Explain two approaches: Greedy and Beam-Search.
	\item Explain why greedy might not give satisfactory results.
\end{itemize}
\paragraph{Soft-Attention Mechanism}
\begin{itemize}
	\item Draw link to "human" attention mechanism
	\item Show how this works on a more formal level.
\end{itemize}
